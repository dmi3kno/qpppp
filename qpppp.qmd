---
title: Quantile-parameterized distributions for expert knowledge elicitation
format:
  tandf-pdf:
    keep-tex: true  
  tandf-html: default
author:
  - name: Dmytro Perepolkin
    affiliations:
      - ref: CEC
    orcid: 0000-0003-2402-304X
    email: dmytro.perepolkin@cec.lu.se
    url: https://ddrive.no
  - name: Erik Lindström
    affiliations:
      - ref: MATSTAT
    orcid: 0000-0002-6468-2624
    email: erik.lindstrom@matstat.lu.se
  - name: Ullrika Sahlin
    email: ullrika.sahlin@cec.lu.se
    affiliations:
      - ref: CEC
    orcid: 0000-0002-2932-6253
affiliations:
  - id: CEC
    name: Lund University
    department: Centre for Environmental and Climate Science
    address: Sölvegatan 37
    city: Lund
    country: Sweden
    postal-code: 223 62
  - id: MATSTAT
    name: Lund University
    department: Centre for Mathematical Sciences
    address: Sölvegatan 18
    city: Lund
    country: Sweden
    postal-code: 223 62
abstract: |
  This paper provides a comprehensive overview of quantile-parameterized distributions (QPDs) as a tool for capturing expert predictions and parametric judgments. We survey a range of methods for constructing distributions that are parameterized by a set of quantile-probability pairs and describe an approach to generalizing them to enhance their tail flexibility. Furthermore, we delve into the extension of QPDs to the multivariate setting, surveying the approaches to construct bivariate distributions, which can be adopted to obtain distributions with quantile-parameterized margins. Through this review and synthesis of the previously proposed methods, we aim to enhance the understanding and utilization of QPDs in various domains.
keywords: 
  - quantile functions
  - quantile-parameterized distributions
  - expert knowledge elicitation
  - statistical distributions
bibliography: qpppp-article.bib  
---

```{r} 
#| label: bibtx
#| include: false 
#| echo: false

if (!interactive()) 
  rbbt::bbt_write_bib('qpppp-article.bib', translator='bibtex', overwrite = TRUE)
```

```{r}
#| label: setup
#| message: false

options(tidyverse.quiet = TRUE)
library(tidyverse)
library(posterior)
library(bayesplot)
library(qpd)
library(kableExtra)
library(ggExtra)
library(GGally)
library(patchwork)
library(targets)
library(cmdstanr)
library(nnls)

library(rvinecopulib)

knitr::opts_chunk$set(dev = "cairo_pdf")
extrafont::loadfonts(device = "pdf",quiet=TRUE)
bayesplot::color_scheme_set("viridisE")
bayesplot::bayesplot_theme_set(
  hrbrthemes::theme_ipsum_rc(grid = FALSE)
  )

```


# Introduction

Judgment plays a crucial role in transforming raw data into meaningful insights. For judgment to be useful, it needs to be translated into the language of mathematical models and assumptions. These models are designed to capture the expert's understanding of the world, including the causal links between relevant entities. The models serve as a representation of this understanding, while also accounting for any limitations in knowledge, which are treated as uncertainties. The process of elicitation involves translating the qualitative understanding of the problem at hand into quantitative models that can provide valuable insights.

## Past research {.unnumbered}

Most of the expert elicitation protocols described in the literature  [@hanea2021ExpertJudgementRisk;@gosling2018SHELFSheffieldElicitation;@ohagan2006UncertainJudgementsEliciting; @hemming2018PracticalGuideStructured; @morgan2014UseAbuseExpert; @welsh2018MoreorlessElicitationMOLE; @spetzler1975ProbabilityEncodingDecision] encode expert judgments about the parameter or quantity of interest as an ordered set of quantiles with corresponding probabilities. This typically includes measures such as the median and the upper and lower quartiles. Assessors are then encouraged to select a probability distribution that reasonably fits the elicited quantile-probability pairs and validate the choice with the expert [@gosling2018SHELFSheffieldElicitation]. A distribution is selected from a predefined set of "simple and convenient" distributions [@ohagan2006UncertainJudgementsEliciting] with boundedness that accounts for the nature of the elicited quantity. 

Several specialized distributions have been developed to facilitate smooth interpolation of probabilistic assessments. These distributions, parameterized by quantile-probability pairs, ensure that the elicited QPPs are exactly preserved [@keelin2011QuantileParameterizedDistributions; @powley2013QuantileFunctionMethods;@keelin2016MetalogDistributions; @hadlock2017QuantileparameterizedMethodsQuantifying; @wilson2023ReconciliationExpertPriors]. Quantile-parameterized distributions are particularly valuable thanks to the interpretability of their parameters. By leveraging the elicited quantiles, these distributions enable precise capturing of expert knowlegde while maintaining a high level of flexibility in modeling.

We believe that the primary utility of QPDs lies in their ability to simplify the specification of probability distributions for model parameters, known as *prior elicitation* [@mikkola2021PriorKnowledgeElicitation]. However, these same distributions can also be employed to describe an expert's predictions for the next observation, referred to as *predictive elicitation* [@winkler1980PriorInformationPredictive; @kadane1980PredictiveStructuralMethods; @akbarov2009ProbabilityElicitationPredictive; @hartmann2020FlexiblePriorElicitation], or to capture both uncertainty and variability through a two-dimensional probability distribution in *hybrid elicitation* [@perepolkin2021HybridElicitationIndirect]. 

This review paper aims to introduce quantile-parameterized distributions (QPDs) to a wide readership. The literature review and the findings are presented through the perspective of quantile functions, building upon the theoretical foundations established by Parzen [@parzen1979NonparametricStatisticalData] and Gilchrist [@gilchrist2000StatisticalModellingQuantile]. The derivatives and inverses for each of the quantile functions discussed in the paper are provided in Appendix A, serving as a valuable reference for future research. Through our comprehensive review and identification of research gaps, we aim to contribute to the development of flexible and extensible distributions that can effectively capture expert knowledge. We hope that our overview of quantile-parameterized distributions will be useful for researchers and practitioners, enabling them to make an informed choice of a distribution suitable for the task. 

## Paper structure {.unnumbered}

In Section 2, we revisit the approaches to quantile parameterization of probability distributions and explore how QPDs can effectively describe expert beliefs regarding model parameters or predictions. Moving on to Section 3, we conduct a comprehensive review and comparison of various continuous univariate QPDs found in the literature. Specifically, we focus on the Myerson distribution and its generalization accommodating different tail thicknesses. To assess the flexibility and behavior of the QPDs, we compare their robust moments. This comparative analysis can guide the selection of an appropriate distribution to characterize the quantity of interest. In Section 4, we delve into several methods for extending univariate distributions to a multivariate setting. These methods include the utilization of standard multivariate distributions [@drovandi2011LikelihoodfreeBayesianEstimation], copulas [@hoff2007ExtendingRankLikelihood], and bivariate quantiles [@nair2023PropertiesBivariateDistributions; @vineshkumar2019BivariateQuantileFunctions]. We show how these techniques can be applied to develop a bivariate version of the Generalized Myerson distribution and demonstrate its application in parametric and predictive elicitation. Finally, in Section 5, we discuss future research directions and potential applications of QPDs in Bayesian analysis.

# Quantile parameterization of probability distributions

In Bayesian data analysis, a fundamental principle is that learning from data requires more than just formulating hypotheses and models. It necessitates the articulation of prior beliefs, expressing existing knowledge in a mathematical form and translating it into a probability distribution for the model parameters.

To accurately translate knowledge into the language of statistical models the encoding distribution needs to be flexible, the process should be transparent, and the results must be interpretable. For continuous distributions, elicitation often consists of capturing a series of quantile-probability pairs (QPPs) [@kadane1998ExperiencesElicitation; @morgan2014UseAbuseExpert], and then fitting a distribution to these pairs [@ohagan2019ExpertKnowledgeElicitation]. However, in practice, the choice of a parametric distribution to fit the elicited QPPs is often influenced by concerns about conjugacy with the selected statistical model that represents the data-generative process (the likelihood) and/or the availability of required distribution functions and fitting algorithms in the software employed. Frequently, the selected distribution possesses fewer parameters than the number of elicited QPPs, which can result in a less-than-perfect fit [@ohagan2019ExpertKnowledgeElicitation]. For instance, it is common to elicit three quantiles (the median along with an upper and lower quartile) and subsequently attempt to fit a normal or lognormal distribution (which features two parameters) to these points.

An alternative approach to characterizing the distribution of predictions or parameters is through quantile-parameterized distributions (QPDs). These distributions are parameterized by the QPPs, allowing the elicited values to directly define the distribution, thereby ensuring a good fit and interpretability of parameters. The QPDs examined in this paper can accommodate a wide range of shapes and boundedness, making them valuable for accurately representing experts' prior beliefs.

Parameterizing distributions using a vector of quantiles is not a novel concept in the scientific community. The earliest mention can be traced back to the *substitution likelihood* proposed by Jeffreys [@jeffreys1939TheoryProbability], which outlines a non-parametric procedure for inferring the median using a set of sample quantiles. Subsequently, similar ideas were further developed in [@boos1986BootstrapMethodsUsing; @lavine1995ApproximateLikelihoodQuantiles; @dunson2005ApproximateBayesianInference].

All the QPDs found in the literature are constructed using the *quantile function*. These distributions are built either by transforming simpler quantile functions or by simultaneous fitting of parameterizing quantiles, as described below.

Let $Y$ be a random variable with a (cumulative) distribution function (CDF) denoted as $F_Y(y\vert\theta)$. The quantile function (QF) $Q_Y(u\vert\theta)$ for $Y$ is defined as

$$
Q_Y(u\vert\theta)=\inf\{y:F_Y(y\vert\theta)\geq u\}, \; u\in[0,1]
$$

Here, $\theta$ represents the distribution parameter, and the subscript $_Y$ indicates that the depth $u$ corresponds to the random variable $Y$.

Both the CDF and the QF are considered equally valid ways of defining a distribution [@tukey1965WhichPartSample]. For a quantile function that is right-continuous and strictly increasing over the support of $Y$, the quantile function $Q_Y(u)$ is simply the inverse of the distribution function, denoted as $Q_Y(u\vert\theta)=F_Y^{-1}(u\vert\theta)$. Therefore, the quantile function is often referred to as the *inverse CDF*.

The derivative of the quantile function, known as the *quantile density function* (QDF), is denoted as $q(u) = \frac{dQ(u)}{du}$. It is reciprocally related to the probability density function (PDF) $f(x)$, such that $f(Q(u))q(u) = 1$. The quantity $f_Y(Q_Y(u\vert\theta))=[q_Y(u\vert\theta)]^{-1}$ is referred to as the *density quantile* function [@parzen1979NonparametricStatisticalData] or *p-pdf* [@gilchrist2000StatisticalModellingQuantile]. The relationships between these functions are concisely illustrated in the probability function Möbius strip (@fig-moebius-chart).

```{r}
#| label: fig-moebius-chart
#| fig-cap: "Möbius strip of probability functions [@perepolkin2023TenetsQuantilebasedInference]"
#| fig-align: center
#| out-width: "40%"
rsvg::rsvg_png("img/moebius-loop(1).svg", "img/moebius-loop(1).png", width=600, height = 600)
knitr::include_graphics("img/moebius-loop(1).png", dpi=200)
```

Although many of the distributions discussed in Section 3 have closed-form cumulative distribution functions (CDFs) and probability density functions (PDFs), the functional form of the quantile function (QF) is often simpler and can be reasoned about in terms of other quantile functions, following *Gilchrist's QF transformation rules* summarized in @tbl-qf-trans [@gilchrist2000StatisticalModellingQuantile]. This table presents the addition, linear combination, and multiplication rules, which involve two quantile functions $Q_1$ and $Q_2$. We will refer to these three rules as *Gilchrist combinations*, as they represent valid ways to combine quantile functions to create new quantile functions.

```{r}
#| label: tbl-qf-trans
#| tbl-cap: "Gilchrist's quantile function transformation rules [@gilchrist2000StatisticalModellingQuantile]"
#| warning: false
#| error: false
#| message: false
#| echo: false

qf_tbl <- tibble::tribble(
  ~`Original QF`,      ~`Rule`,        ~`Resulting QF`,         ~`Resulting variable`,
"$Q_Y(u)$",      "Reflection rule",          "$-Q(1-u)$",           "QF of -Y",
"$Q_Y(u)$",      "Reciprocal rule",         "$1/Q(1-u)$",        "QF of $1/Y$",
"$Q_1(u), Q_2(u)$",      "Addition rule", "$Q_1(u)+Q_2(u)$",         "valid QF",
"$Q_1(u), Q_2(u)$",  "Linear combination rule", "$aQ_1(u)+bQ_2(u)$",   "valid QF for $a,b>0$",
"$Q_1(u),Q_2(u)>0$",  "Multiplication rule", "$Q_1(u)Q_2(u)$",   "valid QF",
"$Q_Y(u)$", "Q-transformation", "$T(Q_Y(u))$",   "QF of $T(Y)$,\n $T(Y)$ non-decreasing",
"$Q_Y(u)$", "p-transformation", "$Q_Y(H(u))$", "p-transformation of $Q_Y(u)$,\n $H(u)$ non-decreasing"
)
qf_tbl$Rule <- kableExtra::linebreak(qf_tbl$Rule)
qf_tbl$`Resulting variable` <- kableExtra::linebreak(qf_tbl$`Resulting variable`)

kableExtra::kbl(qf_tbl,escape=FALSE, format = "latex", booktabs = TRUE,
                caption = "", position="ht") %>% 
  kableExtra::kable_styling(full_width = FALSE) %>% 
  kableExtra::column_spec(2, width="3cm") %>% 
  kableExtra::column_spec(3, width="2.5cm") %>% 
  kableExtra::column_spec(4, width="3cm") 
```

The quantile-parameterized distributions described in this paper can be categorized into two groups based on their construction method. The first group comprises distributions that are *directly* parameterized by the quantile-probability pairs (QPPs). This group includes the Myerson distribution [@myerson2005ProbabilityModelsEconomic], and the Johnson Quantile-Parameterized Distribution [@hadlock2017JohnsonQuantileParameterizedDistributions; @hadlock2019GeneralizedJohnsonQuantileParameterized]. These distributions are constructed by reparameterizing or transforming existing distributions, following Gilchrist rules (@tbl-qf-trans). The transformations used to construct them are detailed in the next section.

The other group of distributions is *indirectly* parameterized by the QPPs. They require a fitting step where the quantile-probability pairs are translated into distribution parameters, usually through optimization or least-squares methods. This group includes the Simple Q-Normal [@keelin2011QuantileParameterizedDistributions], Metalog [@keelin2016MetalogDistributions], quantile mixtures [@peng2023MixtureQuantilesEstimated], the variant of the Generalized Lambda Distribution (GLD) by Chalabi et al [@chalabi2012FlexibleDistributionModeling], and the quantile-parameterized Triangular (Two-Sided Power) distribution by Kotz and van Dorp [@kotz2004BetaOtherContinuous]. The fitting methods for each of these distributions is described in the respective sub-sections below.

# Univariate quantile-parameterized distributions

In this section, we review various continuous univariate quantile-parameterized distributions found in the literature. We then discuss the generalized form for these distributions, based on the variations of these QPDs appearing in the literature. For each distribution, we present its quantile function and discuss the parameterization and feasibility conditions. The derivative and inverse of each distribution can be found in Appendix A.

## Myerson distribution

One of the earliest examples of a distribution parameterized by quantiles is the *generalized log-normal* distribution defined by the median and the upper and lower quartiles proposed by [@myerson2005ProbabilityModelsEconomic]. It relies on a transformation of the normal quantile function.

The Myerson distribution can be viewed as parameterized by three quantile values $\{q_1, q_2, q_3\}$, which correspond to the cumulative probabilities $\{\alpha, 0.5, 1-\alpha\}$. These quantiles are symmetrical around the median and are defined by the tail parameter $0<\alpha<0.5$. This type of parameterization is known as the Symmetric Percentile Triplet (SPT, $\alpha$-level SPT or $\alpha$-SPT) and is also used in several other quantile-parameterized distributions that we will describe below. The Myerson quantile function is

$$
\begin{gathered}
\rho=q_3-q_2;\; 
\beta=\frac{\rho}{q_2-q_1};\;
\kappa(u)=\frac{S(u)}{S(1-\alpha)}\\
Q_Y(u \vert q_1,q_2,q_3,\alpha)=
\begin{cases}
q_2+\rho\frac{\beta^{\kappa(u)}-1}{\beta-1}, \quad &\beta \neq 1\\
q_2+\rho\kappa(u), \quad &\beta =1
\end{cases}
\end{gathered}
$$

Here, $u$ represents the depth of the observations of the random variable $Y$ given the parameterizing $\alpha$-SPT $\{q_1, q_2, q_3, \alpha\}$, with $0 < \alpha < 0.5$. The parameter $\rho$ is the *upper p-difference*, and $\beta$ is the ratio of the inter-percentile ranges, known as the *skewness ratio* [@gilchrist2000StatisticalModellingQuantile, p.72]. The *kernel* quantile function $S(u)$ is equal to the quantile function of the standard normal distribution, also referred to as the probit, defined as $S(u) = \Phi^{-1}(u)$. The formulas for the derivative and the inverse quantile function of the Myerson QPD can be found in Appendix A.

It is important to note that while the Myerson distribution includes the normal distribution as a special case when the skewness parameter $\beta = 1$, it can exhibit right-skewness or left-skewness for other values of $\beta$. In the symmetrical case, the range of the quantile function is $(-\infty, \infty)$. For the right-skewed distribution ($\beta > 1$), the range is $(q_2 - \frac{\rho}{\beta - 1}, \infty)$, and for the left-skewed distribution ($0 < \beta < 1$), the range is $(-\infty, q_2 - \frac{\rho}{\beta - 1})$. The limiting case of the skewed Myerson distribution $\lim_{u \rightarrow 0} Q_Y(u\vert\theta)$ for $\beta > 1$ (and the other limit for $0 < \beta < 1$) possesses some important properties that we discuss in Section 3.3 below.

The basic quantile function [@gilchrist2000StatisticalModellingQuantile; @lampasi2008AlternativeApproachMeasurement] underlying the Myerson distribution is a simple probit, $S(u) = \Phi^{-1}(u)$, transformed using the exponentiation function $T(x) = \beta^{x}$, where $\beta > 0$ represents the skewness ratio [@gilchrist2000StatisticalModellingQuantile]. The quantile parameterization is facilitated by $\kappa(u)$, which takes values $\{-1,0,1\}$ for the three quantiles $\{q_1, q_2, q_3\}$, such that $Q(\alpha) = q_1$, $Q(0.5) = q_2$, and $Q(1 - \alpha) = q_3$.

## Johnson Quantile-Parameterized Distribution

Hadlock and Bickel [@hadlock2017QuantileparameterizedMethodsQuantifying] reviewed the existing quantile-parameterized distributions and proposed the quantile parameterization of the Johnson SU family of distributions [@johnson1994ContinuousUnivariateDistributions]. In their paper, Hadlock and Bickel [@hadlock2017JohnsonQuantileParameterizedDistributions] presented two versions of the distribution: the bounded (J-QPD-B) and the semi-bounded (J-QPD-S), both parameterized by an SPT $\{q_1, q_2, q_3, \alpha\}$ and the bound(s).

The J-QPD-B distribution is obtained by applying the inverse-probit transformation to the Johnson SU quantile function $Q_{SU}(u) = \xi + \lambda\sinh(\delta(S(u) + \gamma))$, where $\delta$ and $\gamma$ are two shape parameters. This function is then rescaled to the compact interval $[l_b, u_b]$. The J-QPD-B quantile function is

$$
\begin{gathered}
Q_B(u\vert q_1, q_2, q_3, \alpha)=
\begin{cases}
l+(u_b-l_b)S^{-1}(\xi+\lambda\sinh(\delta(S(u)+nc))), \quad &n\neq0\\
l+(u_b-l_b)S^{-1}\left(B+\left(\frac{H-L}{2c}\right)S(u)\right), \quad &n=0
\end{cases}
\end{gathered}
$$

where

$$
\begin{gathered}
S(u)=\Phi^{-1}(u); \quad c=S(1-\alpha);\\
L=S\left(\frac{q_1-l_b}{u_b-l_b}\right); \quad  B=S\left(\frac{q_2-l_b}{u_b-l_b}\right);\\
H=S\left(\frac{q_3-l_b}{u_b-l_b}\right); \quad n=\text{sgn}(L+H-2B)\\
\xi=\begin{cases}L, \quad n=1,\\
B, \quad n=0,\\
H, \quad n=-1,\end{cases}\\
\delta=\frac{1}{c}\cosh^{-1}\left(\frac{H-L}{2\min(B-L,H-B)}\right)\\
\lambda=\frac{H-L}{\sinh(2\delta c)}
\end{gathered}
$$

```{r}
#| label: fig-jqpd1
#| fig-width: 12 
#| fig.height: 5 
#| warning: false
#| fig-cap: "Fitted J-QPD-B (left) and J-QPD-S (right) distribution for prevalence at origin and total trade flow, respectively" 
#| out-width: "100%"
#| fig-align: 'center'

p_prev <- tibble::tibble(
  probs = c(1, 25, 50, 75, 99),
  SPT=c("0.01-SPT", "0.25-SPT", "0.25-SPT", "0.25-SPT", "0.01-SPT"),
  A0=c(1, 10, 20, 30, 50)
) %>% mutate(across(where(is.numeric), ~.x/100))

p_trade <- tibble::tibble(
  probs = c(0.01, 0.25, 0.50, 0.75, 0.99),
  SPT=c("0.01-SPT", "0.25-SPT", "0.25-SPT", "0.25-SPT", "0.01-SPT"),
  A0=c(3,9,13,16, 20)
) %>% mutate(A0=A0*1e3)

bounded_df <- tibble::tibble(
  p = make_pgrid(300),
  `q_0.25-SPT` = qJQPDB(p, p_prev$A0[2], p_prev$A0[3], p_prev$A0[4], alpha=p_prev$probs[2], lower = 0, upper = 1),
  `q_0.01-SPT` = qJQPDB(p, p_prev$A0[1], p_prev$A0[3], p_prev$A0[5], alpha=p_prev$probs[1], lower = 0, upper = 1),
  `q_Beta` = qbeta(p, 1.6517, 5.9871)) %>% 
  pivot_longer(-p, names_to= c("variable","SPT"), names_sep = "_",
                   values_transform = as.numeric) %>% 
  mutate(Distribution=ifelse(SPT=="Beta", "Beta", "J-QPD-B")) 

semibounded_df <- tibble::tibble(
  p = make_pgrid(300),
  `q_0.25-SPT` = qJQPDS(p, p_trade$A0[2], p_trade$A0[3], p_trade$A0[4], alpha=p_trade$probs[2], lower = 0),
  `q_0.01-SPT` = qJQPDS(p, p_trade$A0[1], p_trade$A0[3], p_trade$A0[5], alpha=p_trade$probs[1], lower = 0),
  `q_Beta` = 20411*qbeta(p, 2.3853, 1.5285)) %>% 
  pivot_longer(-p, names_to= c("variable","SPT"), names_sep = "_",
                   values_transform = as.numeric) %>% 
  mutate(Distribution=ifelse(SPT=="Beta", "Beta", "J-QPD-S")) 


p1 <- ggplot(bounded_df)+
  geom_line(aes(x=p, y=value, color= fct_rev(SPT), linetype=fct_rev(Distribution)))+
  geom_point(data=pivot_longer(p_prev,-c(probs, SPT), 
                               names_to = "variable",
                               values_transform = as.numeric),
      aes(x=probs, y=value, color=fct_rev(SPT))) +
  scale_y_continuous(limits = c(0,0.55))+
  hrbrthemes::theme_ipsum_rc(grid_col = "grey90")+
  ggthemes::scale_color_colorblind()+
  labs(color="SPT", y="Proportion of infested fruit",
       subtitle="Prevalence at origin",
       x="Probability", linetype="Distribution")+guides(linetype = "none")

p2<- ggplot(semibounded_df)+
  geom_line(aes(x=p, y=value, color= fct_rev(SPT), linetype=fct_rev(Distribution)))+
  geom_point(data=pivot_longer(p_trade,-c(probs, SPT), 
                               names_to = "variable",
                               values_transform = as.numeric),
      aes(x=probs, y=value, color=fct_rev(SPT))) +
  hrbrthemes::theme_ipsum_rc(grid_col = "grey90")+
  scale_y_continuous(label=scales::label_number())+
  ggthemes::scale_color_colorblind()+
  labs(color="SPT", y="tons of citrus fruit per year",
       subtitle="Trade flow (tons of citrus fruit), tons/year",
       x="Probability", linetype="Distribution")+guides(linetype = "none")
wrap_plots(p1,p2, guides = "collect")
```

The left panel in @fig-jqpd1 showcases the J-QPD-B quantile function, which is parameterized using 0.25-SPT and 0.01-SPT assessments of the proportion of fruit infested with *Citripestis sagittiferella*, as elicited by [@efsa2023RiskAssessmentCitripestis]. The dashed line represents the Beta distribution fitted by the authors. The J-QPD-B, being parameterized by an SPT, effectively captures three of the five parameterizing quantiles, while the Beta distribution only provides an approximation. Finding parameters of Beta distribution requires an optimization step.

The J-QPD-S distribution is a semi-bounded variant of the distribution that employs exponentiated hyperbolic arcsine transformations of the Johnson's SU quantile function  [@hadlock2017JohnsonQuantileParameterizedDistributions]

$$
\begin{gathered}
Q_S(u\vert q_1, q_2, q_3, \alpha)=\begin{cases}
l_b+\theta\exp\left(\lambda\sinh\left(\sinh^{-1}(\delta S(u))+\sinh^{-1}(nc\delta)\right)\right), \quad &n \neq 0\\
l_b+\theta\exp\left(\lambda\delta S(u)\right), \quad &n=0
\end{cases}
\end{gathered}
$$

where

$$
\begin{gathered}
S(u)=\Phi^{-1}(u); \quad c=S(1-\alpha);\\
L=\ln(q_1-l_b); \quad  B=\ln(q_2-l_b);\\
H=\ln(q_3-l_b); \quad n=\text{sgn}(L+H-2B)\\
\theta=\begin{cases}
q_1-l_b, \quad n=1,\\
q_2-l_b, \quad n=0,\\
q_3-l_b, \quad n=-1,\end{cases}\\
\delta=\frac{1}{c}\sinh\left(\cosh^{-1}\left(\frac{H-L}{2\min(B-L,H-B)}\right)\right)\\
\lambda=\frac{1}{\delta c}\min(H-B, B-L)
\end{gathered}
$$

When $n=\text{sgn}(L+H-2B)$ evaluates to zero, he resulting distribution is a lognormal distribution with parameters  $\mu=\ln(\theta)=\ln(q_2-l_b)$ and $\sigma=\lambda\delta=(H-B)/c$. This distribution has support on the interval $[l_b,\infty]$.

The right panel in @fig-jqpd1 depicts the J-QPD-S quantile function, which is parameterized using 0.25-SPT and 0.01-SPT assessments of the total trade flow for citrus fruit imported by the EU from Indonesia, Malaysia, Thailand, and Vietnam in tons/year [@efsa2023RiskAssessmentCitripestis].


## Generalisations of QPDs

### Generalized Johnson Quantile-Parameterized Distribution

Hadlock and Bickel [@hadlock2019GeneralizedJohnsonQuantileParameterized] introduced the *generalized* version of the Johnson Quantile-Parameterized distribution system, denoted as G-QPD, by replacing the Normal distribution in the core of the Johnson SU quantile function with the quantile functions of the logistic and Cauchy distributions.

The generalized quantile function (QF) shares similarities with the probit-based distribution described earlier, with $S(u)$ defined as the quantile function of either the logistic or Cauchy distribution.

The standard quantile function and distribution function of the logistic distribution are given by:

$$
S(u)= \ln\left(\frac{u}{1-u}\right);\quad S^{-1}(y)=[\exp(-y)+1]^{-1}
$$

The standard quantile function and distribution function of the Cauchy distribution are given by:

$$
S(u)= \tan\left[\pi\left(u-\frac{1}{2}\right)\right];\quad S^{-1}(y)=\frac{1}{ \pi}\arctan(y)+\frac{1}{2}
$$

Hadlock and Bickel [@hadlock2019GeneralizedJohnsonQuantileParameterized] show that the *kernel* quantile function $S(u)$ can be any standardized ($S(0.5)=0$), symmetrical ($s(u)=s(1-u)$), and unbounded ($S(u)\in(-\infty;\infty)$) quantile function with a smooth quantile density $dS(u)/du=s(u)$. The authors further showed that if $S(u)$ and $S^{-1}(y)$ are expressible in closed-form, the quantile function and distribution function of G-QPD will also be closed-form. 

For the *logistic* kernel, the G-QPD-S represents the generalized log-logistic distribution, characterized by two shape parameters, $\lambda$ and $\delta$. For the Cauchy kernel, the G-QPD-S corresponds to the shifted log-Cauchy distribution [@hadlock2019GeneralizedJohnsonQuantileParameterized].


### Generalized Myerson distributions

Following the approach in Hadlock and Bickel [@hadlock2019GeneralizedJohnsonQuantileParameterized], Myerson distribution can be generalized by substituting the Normal kernel quantile function $S(u)=\Phi^{-1}(u)$ with an alternative symmetrical quantile function based on the depth $u$. Below, we discuss possible kernels and the resulting distributions:

**Logit-Myerson distribution**. Recently Wilson et al [@wilson2023ReconciliationExpertPriors] reparameterized *log-logistic distribution* in terms of a Symmetric Percentile Triplet. Even though the authors do not recognize it as such, the resulting quantile-parameterized distribution is a Myerson distribution with logit kernel QF $S(u)=\ln\left(\frac{u}{1-u}\right)$).

There could be several reasons why one might prefer the logit function over the probit function [@berkson1951WhyPreferLogits]. For example, distribution based on logit may exhibit greater numerical stability due to its simple closed-form quantile function, which does not rely on numerical approximation during sampling. Logit-Myerson distribution displays slightly heavier tails compared to the standard (probit-based) Myerson distribution (@fig-gmyerson-qfdqf-plot1).

**Sech-Myerson distribution**. Following the same principle adopted by Wilson et al. [@wilson2023ReconciliationExpertPriors] a variant of Myerson distribution may be created using the hyperbolic secant quantile function:

$$
S(u)=\ln\left[\tan\left(\frac{\pi}{2}u\right)\right]
$$

The Sech-Myerson distribution possesses thicker tails than the Logit-Myerson distribution for the same parameterizing SPT $\{-5,4,16, 0.25\}$ (@fig-gmyerson-qfdqf-plot1). In @sec-compareqf, we conduct a comparative analysis of different variations of the Generalized Myerson distribution alongside their parametric counterparts and other quantile distributions.

```{r} 
#| label: fig-gmyerson-qfdqf-plot1
#| warning: false
#| error: false
#| message: false
#| echo: false
#| fig-width: 12
#| fig-height: 5
#| out-width: "100%"
#| fig-align: 'center'
#| fig-cap: "Quantile function and quantile density of Generalized Myerson Distributions"

fMMyerson <- function(u, q1,q2,q3, alpha=0.25, sfun=stats::qnorm, dsfun=qpd::fnorm){
  r <- (q3-q2)
  bt <- r/(q2-q1)
  qn1ma <- sfun(1-alpha)
  k <- sfun(u)/qn1ma
  if(bt==1){
    res <- r*dsfun(u)/qn1ma
  }else{
    res <- r*(bt^k)*log(bt)*dsfun(u)/(bt-1)/qn1ma
  }
  res
}

qsech <- function(u){
  log(tan(pi/2*u))
}

fsech <- function(u){
  pi*(1+tan(0.5*pi*u)^2)/(2*tan(0.5*pi*u))
}

qs <- c(-5,4,16)
alph <- 0.25

mm_data <- tibble::tibble(
   p_grd=qpd::make_pgrid(300),
   `qf_(Probit)`=qpd::qMMyerson(p_grd,qs[1],qs[2],qs[3],alpha=alph, sfun=qnorm),
   `df_(Probit)`=1/fMMyerson(p_grd,qs[1],qs[2],qs[3],alpha=alph, sfun=qnorm, dsfun=qpd::fnorm),
   qf_Logit=qpd::qMMyerson(p_grd, qs[1], qs[2], qs[3], alpha=alph, sfun=stats::qlogis),
   df_Logit=1/fMMyerson(p_grd, qs[1], qs[2], qs[3],alpha=alph, sfun=qlogis, dsfun=qpd::flogis),
   qf_Sech=qpd::qMMyerson(p_grd, qs[1], qs[2], qs[3], alpha=alph, sfun=qsech),
   df_Sech=1/fMMyerson(p_grd, qs[1], qs[2], qs[3], alpha=alph, sfun=qsech, dsfun=fsech)) %>%
  pivot_longer(-p_grd, names_to="name", values_to = "value") %>% 
  separate(name, into=c("type", "distribution"), sep="_") %>% 
  mutate(distribution=paste0(distribution, "-Myerson")) %>% 
  pivot_wider(id_cols = c(p_grd,distribution), names_from = "type", values_from = "value")

p1 <- ggplot()+
  geom_line(data=mm_data, aes(x=p_grd, y=qf, color=distribution))+
  geom_point(aes(y=qs, x=c(alph, 0.5, 1-alph)))+
  hrbrthemes::theme_ipsum_rc(grid_col = "grey80")+
  coord_cartesian(ylim=c(-40,60))+
  ggthemes::scale_color_colorblind()+
  labs(x="u", y="Q(u)", color="Distribution")

p2 <- mm_data %>% 
  ggplot()+
  geom_line(aes(x=qf, y=df, color=distribution))+
  #geom_vline(xintercept=qs, color="grey80", linetype=2)+
  hrbrthemes::theme_ipsum_rc(grid_col = "grey80")+
  coord_cartesian(xlim=c(-40,60))+
  ggthemes::scale_color_colorblind()+
  labs(x="Q(u)", y="1/q(u)",color="Distribution")

patchwork::wrap_plots(p1, p2, guides = "collect")
```

Theoretically, there is an infinite range of quantile function (QF) kernels that can be utilized to generate new variations of the Generalized Myerson distribution. These candidate kernel distributions can even include shape parameters, as long as the resulting $S(u)$ remains standardized, symmetrical, and unbounded, as specified above. For instance, it is possible to incorporate the basic QF of the Tukey Lambda distribution $S(u\vert\lambda)=u^\lambda-(1-u)^\lambda$ for a fixed $\lambda \neq 0$, or the Cauchy distribution $S(u)=\tan[\pi(u-0.5)]$, as employed by [@hadlock2019GeneralizedJohnsonQuantileParameterized]. However, it is important to note that not all standard quantile functions are created equal. To illustrate the issue of unreliable kernels, let us consider Myerson distributions based on the Cauchy and Tukey Lambda quantile functions (for $\lambda=-0.5$). As can be observed in @fig-gmyerson-qfdqf-plot2, the density of Generalized Myerson distribution with these kernels exhibits unexpected spike near the lower bound.

```{r}
#| label: fig-gmyerson-qfdqf-plot2
#| warning: false
#| error: false
#| message: false
#| echo: false
#| fig-width: 12
#| fig-height: 5
#| out-width: "100%"
#| fig-align: 'center'
#| fig-cap: "Quantile function and quantile density of Generalized Myerson Distributions with unreliable kernels"

fMMyerson <- function(u, q1,q2,q3, alpha=0.25, 
                      sfun=stats::qnorm, dsfun=qpd::fnorm, ...){
  r <- (q3-q2)
  bt <- r/(q2-q1)
  qn1ma <- sfun(1-alpha, ...)
  k <- sfun(u, ...)/qn1ma
  if(bt==1){
    res <- r*dsfun(u, ...)/qn1ma
  }else{
    res <- r*(bt^k)*log(bt)*dsfun(u, ...)/(bt-1)/qn1ma
  }
  res
}

qsech <- function(u){
  log(tan(pi/2*u))
}

fsech <- function(u){
  pi*(1+tan(0.5*pi*u)^2)/(2*tan(0.5*pi*u))
}


fcauchy <- function(u){
  pi*(1+tan(pi*(u-0.5))^2)
}

qs <- c(-5,4,16)
alph <- 0.25

mm_data <- tibble::tibble(
  p_grd=qpd::make_pgrid(300),
  qf_Sech=qpd::qMMyerson(p_grd, qs[1], qs[2], qs[3], alpha=alph, sfun=qsech),
  df_Sech=1/fMMyerson(p_grd, qs[1], qs[2], qs[3],alpha=alph, sfun=qsech, dsfun=fsech),
  qf_Cauchy=qpd::qMMyerson(p_grd, qs[1], qs[2], qs[3], alpha=alph, sfun=stats::qcauchy),
  df_Cauchy=1/fMMyerson(p_grd, qs[1], qs[2], qs[3], alpha=alph, sfun=stats::qcauchy, dsfun=fcauchy),
  qf_Tukey=qpd::qtukeyMyerson(p_grd, qs[1], qs[2], qs[3], alpha=alph, tlambda=-0.5),
  df_Tukey=qpd::dqtukeyMyerson(p_grd, qs[1], qs[2], qs[3], alpha=alph, tlambda=-0.5)
) %>% pivot_longer(-p_grd, names_to="name", values_to = "value") %>% 
  separate(name, into=c("type", "distribution"), sep="_") %>% 
  mutate(distribution=paste0(distribution, "-Myerson")) %>% 
  pivot_wider(id_cols = c(p_grd,distribution), names_from = "type", values_from = "value")

p1 <- ggplot()+
  geom_line(data=mm_data, aes(x=p_grd, y=qf, color=distribution))+
  geom_point(aes(y=qs, x=c(alph, 0.5, 1-alph)))+
  coord_cartesian(ylim=c(-40,60))+
  hrbrthemes::theme_ipsum_rc(grid_col = "grey80")+
  ggthemes::scale_color_colorblind()+
  labs(x="u", y="Q(u)", color="Distribution")


p2 <- mm_data %>% 
  ggplot()+
  geom_line(aes(x=qf, y=df, color=distribution))+
  #geom_vline(xintercept=qs, color="grey80", linetype=2)+
  coord_cartesian(xlim=c(-40,60), ylim=c(0,0.03))+
  hrbrthemes::theme_ipsum_rc(grid_col = "grey80")+
  ggthemes::scale_color_colorblind()+
  labs(x="Q(u)", y="1/q(u)",color="Distribution")

patchwork::wrap_plots(p1, p2, guides = "collect")

```

While all right-skewed Generalized Myerson distributions are bounded on the left at $\lim_{u\rightarrow0}Q(u\vert\theta)=q_2-\rho\frac{1}{\beta-1}$ regardless of the kernel used, the quantile density at the left limit $\lim_{u\rightarrow0}[q(u\vert\theta)]^{-1}$ is not independent of the kernel. Although we can assume that $q(0)=\infty$, the lower tail of the density quantile function $[q(u)]^{-1}$ may exhibit a curling effect for certain kernels, resulting in an increase in density for lower values of $u$. This effect is caused by the non-monotonic behavior of the quantile convexity function $c(u)=dq(u)/du$. This can be easily verified by taking the second derivative of $\beta^{S(u)}$ for $\beta>0$. While such kernels are mathematically valid and yield a non-decreasing Generalized Myerson QF, we believe that they may be less useful due to the counter-intuitive concentration of density in the bounded tail. Consequently, we do not recommend using Cauchy or Tukey Lambda kernels in practical applications.


## Simple Q-Normal, Metalog distributions

An alternative system of quantile-parameterized distributions was proposed by Keelin and Powley [@keelin2011QuantileParameterizedDistributions;@powley2013QuantileFunctionMethods]. This approach relies on the finite Taylor expansion of parameters in the standardized quantile functions. Within this framework, two distributions were introduced: the Simple Q-Normal distribution and the Metalog distribution.

The Simple Q-Normal (SQN) distribution was developed by expanding the parameters in the normal quantile function. Keelin et al. (2011) used this method to express the parameters of the normal quantile function $Q(u\vert\mu,\sigma)=\mu+\sigma z(u)$ as linear functions of the depth $u$. Specifically, $\mu(u)=a_1+a_4u$ and $\sigma(u)=a_2+a_3u$, where $z(u)=\Phi^{-1}(u)$ denotes the standard normal quantile function. Therefore, the quantile function of the SQN distribution can be expressed as follows:

$$
Q(u)= a_1+a_2z(u)+a_3uz(u)+a_4u\\
$$ {#eq-SQNQF}

where $z(u)=\Phi^{-1}(u)$, and $a=\{a_1,a_2,a_3, a_4\}$ represents a vector of parameters. 

Consider a quantile-probability tuple of size 4, denoted as $\{\mathbf{p}, \mathbf{q}\}_4$, which consists of an ordered vector of cumulative probabilities $\mathbf{p}=\{p_1,p_2,p_3, p_4\}$ and an ordered vector of corresponding quantiles $\mathbf{q}=\{q_1,q_2,q_3, q_4\}$. Substituting these vectors into the SQN quantile function for $u$ and $Q(u)$, respectively, we obtain the following matrix equation:

$$
\mathbf{q}=\mathbb Pa
$$ {#eq-sqn-matrix}

where 

$$
\begin{gathered}
\mathbb P=\begin{bmatrix} 1 & z(p_1) & p_1z(p_1) & p_1\\
                1 & z(p_2) & p_2z(p_2) & p_2\\
                1 & z(p_3) & p_3z(p_3) & p_3\\
                1 & z(p_4) & p_4z(p_4) & p_4\end{bmatrix}
\end{gathered}
$$

and $a=\{a_1, a_2, a_3, a_4\}$ represents the parameter vector of the SQN distribution.

The parameter vector $a$ can be obtained by solving the matrix @eq-sqn-matrix, given the 4-element quantile-probability tuple $\{\mathbf{p}, \mathbf{q}\}_4$ [@keelin2011QuantileParameterizedDistributions; @perepolkin2021HybridElicitationIndirect].

The same approach was later employed by [@keelin2016MetalogDistributions] in creating the metalog (meta-logistic) distribution. Starting with the quantile function of the logistic distribution $Q(u\vert\mu,s)=\mu+s\text{logit}(u)$, where $\mu$ corresponds to the mean and $s$ is proportional to the standard deviation $\sigma=s\pi/\sqrt3$, [@keelin2016MetalogDistributions] expanded the parameters $\mu$ and $s$ using a finite Taylor series centered at 0.5. Specifically, $\mu(u)=a_1+a_4(u-0.5)+a_5(u-0.5)^2+\dots$ and $s(u)=a_2+a_3(u-0.5)+a_6(u-0.5)^2+\dots$, where $a_i, \; i = \{1,2,\dots,n\}$ are real constants.

Therefore, the metalog quantile function is:

$$
Q(u)= a_1+a_2\text{logit}(u)+a_3(u-0.5)\text{logit}(u)+a_4(u-0.5)+a_5(u-0.5)^2\cdots,
$$

Given a QPT of size $m$ denoted by $\{\mathbf{p}, \mathbf{q}\}_m$, where $\mathbf{p}$ and $\mathbf{q}$ are ordered vectors of cumulative probabilities and corresponding quantiles, respectively, the vector of coefficients $\mathbf{a}={a_1,\dots,a_m}$ can be determined by solving the matrix equation $\mathbf{q}=\mathbb{P}\mathbf{a}$, where $\mathbf{p}$, $\mathbf{q}$, and $\mathbf{a}$ are column vectors, and  $\mathbb{P}$ is an $m \times n$ matrix:

$$
\begin{gathered}
\mathbb{P} = \left[\begin{array}{lllll}
1  &\text{logit}(p_1) &(p_1-0.5)\text{logit}(p_1) &(p_1-0.5) &\cdots\\
1  &\text{logit}(p_2) &(p_2-0.5)\text{logit}(p_2) &(p_2-0.5) &\cdots\\
   &                  &\vdots\\
1  &\text{logit}(p_m) &(p_m-0.5)\text{logit}(p_m) &(p_m-0.5) &\cdots
\end{array}\right]
\end{gathered}
$$ {#eq-metalogPMatrixeq}

The vector of coefficients $\mathbf{a}$ can be determined as $\mathbf{a}=[\mathbb{P}^{T}\mathbb{P}]^{-1}\mathbb{P}^{T}\mathbf{q}$. If $\mathbb{P}$ is a square matrix, meaning the number of terms $n$ is equal to the size of the parameterizing QPT $m$, the equation can be further simplified to $\mathbf{a}=\mathbb{P}^{-1}\mathbf{q}$. Metalog is said to be *approximated* when the number of quantile-probability pairs used for parameterization exceeds the number of terms in the metalog QF [@keelin2016MetalogDistributions; @perepolkin2021HybridElicitationIndirect].

The SQN and Metalog distributions are families of extended distributions that, in theory, can have an arbitrary number of terms. Keelin [@keelin2016MetalogDistributions] demonstrated the flexibility of the metalog distribution and its ability to approximate arbitrarily complex probability density functions with high precision, given enough terms in the metalog specification. In practice, 10-15 terms are sufficient to approximate the distributional shapes of virtually any complexity [@keelin2021MetalogDistributionsVirtually]. Keelin [@keelin2016MetalogDistributions] introduced the bounded logit-metalog, the semi-bounded log-metalog, and a special case of a 3-term metalog parameterized by $\alpha$-SPT (SPT-metalog).

However, not all combinations of parameters $\mathbf{a}$ in metalog and SQN distributions result in a feasible (non-decreasing) quantile function. For an arbitrary $\mathbf{a}$-vector, feasibility must be checked [@keelin2011QuantileParameterizedDistributions]. In the case of 3-term metalogs, the feasibility conditions are straightforward [@keelin2016MetalogDistributions]. But as the number of terms increases, such conditions become increasingly complex [@keelin2017MetalogDistributionsFeasibility]. Having to deal with such feasibility requirements stands in contrast with QF’s that are constructed using Gilchrist rules @tbl-qf-trans, which guarantee feasibility.

## Quantile mixtures

Recently [@peng2023MixtureQuantilesEstimated] proposed a novel framework for extended quantile-parameterized distributions based on quantile mixtures (not to be confused with CDF/PDF mixtures, [@gilchrist2000StatisticalModellingQuantile, p. 107]). They introduced a formulation in which a QPD quantile function is expressed as a linear combination of $I$ standardized quantile functions, following Gilchrist's *linear combination rule* (@tbl-qf-trans):

$$
G(u\vert\theta)=\sum_{i=0}^I\theta_iQ_i(u)
$$

Here, $Q_i(u)$ represent basis quantile functions for the random variable $Y$ with $Q_0(u)=1$, and $\pmb\theta=\{\theta_0,\theta_1,\dots,\theta_I\}$ is a non-negative parameter vector that determines the contribution of each QF component in the quantile mixture. To compute the coefficients $\pmb\theta$, the system of equations is solved

$$
\mathbf q=\mathbb Q \pmb\theta+\pmb\epsilon
$$

where $\mathbf{q}=\{q_1,q_2,\dots, q_j\}$ is an ordered vector of $J$ parameterizing quantiles, corresponding to an ordered vector of cumulative probabilities $\mathbf{p}=\{p_1,p_2,\dots, p_j\}$, $\pmb\theta$ is a non-negative vector of $I+1$ parameters, $\pmb\epsilon$ is  a $J$-size vector of errors to be minimized, and $\mathbb Q$ is a $J\times(I+1)$ matrix of regression factors

$$
\begin{gathered}
\mathbb{Q} = \left[\begin{array}{lllll}
1  &Q_1(p_1) &Q_2(p_1) &\cdots &Q_I(p_1)\\
1  &Q_1(p_2) &Q_2(p_2) &\cdots &Q_I(p_2)\\
   &\vdots   &\vdots   &\ddots \\
1  &Q_1(p_J) &Q_2(p_J) &\cdots &Q_I(p_J)
\end{array}\right]
\end{gathered}
$$

By ensuring non-negativity of weights ($\theta_i\geq0$), the solution guarantees a proper non-decreasing quantile function. To estimate the values of the vector $\pmb\theta\in\Theta$, the authors suggest using constrained weighted least squares regression with optional regularization. The authors demonstrated that the estimator $\widehat{\pmb\theta}=\underset{\pmb\theta\in\Theta}{\text{argmin}} \left(\frac{1}{J}\sum_{j=1}^Jw_j\mathcal{E}_q(y_j-Q_j\pmb\theta)\right)^{\frac{1}{q}}$, $\mathcal{E}_q(x)=\lvert x \rvert^q$, $w_j>0$, is asymptotically a q-Wasserstein distance estimator, which converges in distribution to a Normal distribution. The paper [@peng2023MixtureQuantilesEstimated] includes the application of the quantile mixture model using a large number of asymmetric t-distributions, and a quantile mixture of Generalized Beta II distributions.

The quantile mixtures method of creating new QPDs guarantees feasibility by construction, while affording nearly infinite flexibility, provided that the component quantile functions are selected from a wide set of distributions of varying shapes. Besides, a QPD constructed as a linear combination of QFs is guaranteed to be unimodal, unless one of the component in the mixture is multimodal [see @gilchrist2000StatisticalModellingQuantile for examples]. In addition, the method proposed by [@peng2023MixtureQuantilesEstimated] offers an advantage of resulting in closed-form quantile function and quantile density function, provided that each of the components can be expressed analytically. Unfortunately, neither asymmetric t-distribution nor Generalized Beta II distribution, used by the authors, has a closed-form QF. However, one can construct a highly flexible quantile function using Gilchrist rules (@tbl-qf-trans) or use one of the existing well-studied QFs discussed in the literature. In @sec-qmexample, we provide an example of using a quantile mixture of diversely-shaped quantile functions to construct a bespoke highly flexible QPD. 

## Other distributions

### Triangular and Two-Sided Power distributions

Several other distributions with at least some parameters mapped to quantiles were proposed, including the reparameterization of the Generalized Lambda Distribution by [@chalabi2012FlexibleDistributionModeling] and the quantile-parameterized triangular (two-sided power) distribution by [@kotz2004BetaOtherContinuous].

Kotz and van Dorp [@kotz2004BetaOtherContinuous] describe the quantile-parameterized version of the triangular distribution [@johnson1997TriangularDistributionProxy]. This bounded distribution is widely used in the finance and insurance industry and is popularized by the \@Risk software package, developed by Palisade [@palisadecorporation2009GuideUsingRISK]. The triangular distribution is parameterized by the two quantiles $q_{a}$ and $q_{b}$, and the mode $m$, subject to the constraint that $a\leq q_a\leq m\leq q_b\leq b$, where $a$ and $b$ represent the lower and upper bounds, respectively. The standard quantile function for the triangular distribution is expressed in terms of the bounds $a$, $b$, and the mode $m$.

$$
\begin{gathered}
Q(u\vert a,m,b)=\begin{cases}
a+\sqrt{u(m-a)(b-a)}, &\quad \text{for } 0\leq u \leq\frac{m-a}{b-a}\\
b-\sqrt{(1-u)(b-m)(b-a)}, &\quad \text{for } \frac{m-a}{b-a}\leq u \leq 1
\end{cases}
\end{gathered}
$$

In [@kotz2004BetaOtherContinuous] the authors show that given the two parameterizing quantile-probability pairs ${q_a,p_a}$ and ${q_b,p_b}$ and the mode value $m$, there exists a unique value of depth $p_a<p<p_b$ corresponding to the root of the function

$$
g(p)=\frac{(m-q_a)(1-\sqrt{\frac{1-p_b}{1-p}})}{(q_b-m)(1-\sqrt{\frac{p_a}{p}})+(m-q_a)(1-\sqrt{\frac{1-p_b}{1-p}})}-p
$$

The root value $p\in (p_a,p_b)$ of the function $g(p)$ can be found using any of the bracketing root-finding algorithms [@perepolkin2023TenetsQuantilebasedInference]. It can then be substituted into the following expressions to find the lower $a$ and upper $b$ limit parameters of the triangular distribution:

$$
\begin{gathered}
a(p) \equiv \frac{q_a-m\sqrt{\frac{p_a}{p}}}{1-\sqrt{\frac{p_a}{p}}}, \quad a(p)<q_a\\
b(p) \equiv \frac{q_b-m\sqrt{\frac{1-p_b}{1-p}}}{1-\sqrt{\frac{1-p_b}{1-p}}}, \quad b(p)>q_b
\end{gathered}
$$

The book [@kotz2004BetaOtherContinuous] provides an algorithm for fitting a four-parameter generalization of the triangular distribution called the Two-Sided Power Distribution (TSP), using three quantile-probability pairs and a mode value. For more information on fitting the Quantile-Parameterized TSP Distribution by quantiles, refer to Section 4.3.3 of [@kotz2004BetaOtherContinuous].

### Generalized Lambda Distribution {#sec-gld}

Chalabi, Scott and Würtz (CSW) [@chalabi2012FlexibleDistributionModeling] proposed an asymmetry-steepness reparameterization of the Generalized Lambda Distribution (GLD) [@freimer1988StudyGeneralizedTukey] with four parameters. This reparameterization involves mapping the location to the median and the scale to the interquartile range (IQR), which corresponds to the first and second robust moments [@kim2004MoreRobustEstimation; @moors1988QuantileAlternativeKurtosis]. 

The reparameterized Generalized Lambda Distribution (CSW GLD) has a quantile function given by

$$
Q(u\vert\tilde\mu,\tilde\sigma,\chi,\xi)=\tilde\mu+\tilde\sigma\frac{S\left(u\vert\chi,\xi\right)-S\left(\frac{1}{2}\vert\chi,\xi\right)}{S\left(\frac{3}{4}\vert\chi,\xi\right)-S\left(\frac{1}{4}\vert\chi,\xi\right)}
$$

where $\tilde\mu,\tilde\sigma,\chi,\xi$ represent the location, scale, asymmetry, and steepness parameters, respectively. The specific form of the basic function $S(u)$ depends on the values of the parameters $\chi$ and $\xi$

$$
S(u\vert\chi,\xi)=
\begin{cases}
\begin{aligned}
&\ln(u)-\ln(1-u),  \quad \text{if }\chi=0,\xi=0.5&\\
&\ln(u)-\frac{1}{2\alpha}\left[(1-u)^{2\alpha}-1\right], \quad \text{if }\chi\neq0,\xi=\frac{1}{2}(1+\chi)&\\
&\frac{1}{2\beta}\left[u^{2\beta}-1\right]-\ln(1-u), \quad \text{if }\chi\neq0,\xi=\frac{1}{2}(1-\chi)&\\
&\frac{1}{\alpha+\beta}\left[u^{\alpha+\beta}-1\right]-\frac{1}{\alpha-\beta}\left[(1-u)^{\alpha-\beta}-1\right], \quad \text{otherwise}
\end{aligned}
\end{cases}
$$

where $\alpha=0.5\frac{0.5-\xi}{\sqrt{\xi(1-\xi)}}$ and $\beta=0.5\frac{\chi}{\sqrt{1-\chi^2}}$. The bounds of the distribution are given by

$$
\begin{gathered}
S(0\vert\chi,\xi)=\begin{cases}
\begin{aligned}
&-\frac{1}{\alpha+\beta},\quad &\text{if }\xi<\frac{1}{2}(1+\chi)\\
&-\infty, \quad &\text{otherwise}
\end{aligned}
\end{cases}\\
S(1\vert\chi,\xi)=\begin{cases}
\begin{aligned}
&\frac{1}{\alpha-\beta},\quad &\text{if }\xi<\frac{1}{2}(1-\chi)\\
&\infty, \quad &\text{otherwise}
\end{aligned}
\end{cases}
\end{gathered}
$$

The CSW GLD can have unbounded, bounded, and semi-bounded support, accommodating a wide range of shapes, including unimodal, monotone, U-shaped, and S-shaped densities [@chalabi2012FlexibleDistributionModeling]. Although the CSW GLD is not strictly parameterized by quantiles, the mapping of the location and scale parameters to the median and IQR makes it a suitable candidate for expert-informed distribution specification.

Several specialized methods have been developed for fitting the GLD to samples [@karian2003ComparisonGLDFitting]. The parameterization of the CSW GLD simplifies the fitting process because two of the four parameters can be directly calculated from the sample: the location parameter is equal to the sample median, and the scale parameter is equal to the interquartile range. The remaining parameters can be estimated using various methods, including robust moment matching, quantile matching, trimmed L-moments, distributional least squares/absolutes, as well as maximum likelihood estimation [@chalabi2012FlexibleDistributionModeling; @gilchrist2000StatisticalModellingQuantile]. The range of feasible values for the steepness and asymmetry parameters can be further reduced with the shape conditions specified in Section 3.5 of [@chalabi2012FlexibleDistributionModeling]. 

Recently, [@dedduwakumara2021EfficientEstimatorParameters] proposed a new method of matching the shape of the GLD distribution to data using the probability density quantile (pdQ) function [@staudte2017ShapesThingsCome]. For the quantile function $Q(v), \; v\in [0,1]$ and the corresponding density quantile function $f(Q(v))=[q(v)]^{-1}$, the pdQ is defined as

$$
f^*(v)=\frac{f(Q(v))}{E\left[f(Q(v))\right]}
$$

The probability density quantile function is defined on the unit square and is independent of the location and scale parameters.

Since integrating the GLD density quantile function is difficult, [@staudte2017ShapesThingsCome, Section 2.2], proposed using the kernel density method to estimate the empirical QDF and, thus, an empirical pdQ for samples from continuous distributions. Fitting the CSW GLD to a sample can be reduced to finding the asymmetry and steepness parameters that minimize

$$
\underset{\chi,\xi}{\text{argmin}}\int_0^1\left[f^*(v, \chi, \xi)-f_{e}^*(v)\right]^2du
$$

where $f^*(v,\chi,\xi)$ is the pdQ of the CSW GLD, and $f^*_e(v)$ is the empirical pdQ of the sample. The authors [@dedduwakumara2021EfficientEstimatorParameters] suggest approximating the integral by a discrete set of depths $v$, replacing the integral with a sum.

## Example {#sec-qmexample}

As an illustration of a faithful approximation of a large number of quantile-probability pairs by QPD, we take 4000 posterior samples (4 chains of 1000 samples each) of one of the random intercepts in the Eight Schools example model included in the `cmdstanr` package [@gabry2022CmdstanrInterfaceCmdStan] in R. The Eight Schools problem [@rubin1981EstimationParallelRandomized] measuring the effectiveness of SAT coaching program in 8 US schools is often used as an example model in introductory classes on Bayesian Statistics. In `cmdstanr` it is modeled using a hierarchical Bayesian model with normal priors for each of the 8 random intercepts `theta`. However, due to the low number of posterior samples and the heterogeneity in the data, the marginal posterior distributions of the intercept parameters `theta` deviate from the Gaussian shape in various ways (@fig-school-thetas). 

An empirical distribution of posterior samples from a Bayesian model can be viewed as a large number of quantile-probability pairs. Although it is unlikely that such number of quantile-probability pairs could ever be elicitable from an expert (in our case 4000), it could still be of interest to approximate such marginal posterior distribution with a highly flexible quantile function, e.g. for the purpose of posterior passing [@brand2019CumulativeScienceBayesian;@pritsker2021ComparingBayesianPosterior]. Closed-form QF expression for the posterior margins would allow reusing it as a prior in a similar model at a later stage. We discuss multivariate extension of this idea in @sec-multivariateqpd.

```{r}
#| label: fig-school-thetas
#| echo: false
#| fig-width: 10
#| fig-height: 5
#| fig-cap: "Posterior distributions of random interecept parameters `theta` in the Eight Schools example model [@gabry2022CmdstanrInterfaceCmdStan]"
#| out-width: "100%"
#| message: false

set.seed(42)
exmpl_fit <- cmdstanr::cmdstanr_example("schools")

exmpl_draws <- exmpl_fit |> as_draws_array()

#variables(exmpl_draws)
# "lp__" "mu" "tau" "theta[1]" "theta[2]" "theta[3]"
# "theta[4]" "theta[5]" "theta[6]" "theta[7]" "theta[8]"

exmpl_draws |>
 subset_draws(variable="theta") |>
 bayesplot::mcmc_dens (facet_args = list(ncol=4))

theta_smpls <- exmpl_draws |>
  extract_variable("theta[5]")

theta_ecfd_df <- theta_smpls |> make_ecdf_df() |> arrange(p)
```


@fig-fitted-qm shows a QPD approximation of the marginal distribution of `theta[5]` using a quantile mixture of standardized (centered at zero and with the scale parameter set to one)  @chalabi2012FlexibleDistributionModeling  Generalized Lambda Distributions (CSW GLD). In order to ensure the diversity of mixture components we generated 400 independent uniformly distributed pairs of the two shape parameters for GLD components using @hubbard2019MultiDimensionalCounterBasedPseudo pseudo random number generator.

We constructed the matrix $\mathbb Q$ above following the method outlined by @peng2023MixtureQuantilesEstimated and used Lawson-Hanson non-negative least squares algorithm (implemented in  `nnls` package [@mullen2023NnlsLawsonhansonAlgorithm] in R) to find the weights for each of the mixture components. The non-zero elements are shows in @fig-gld-comp along with the weights (which become the scale parameters of the quantile mixture components).

```{r}
#| label: fig-chi-xi
#| fig-width: 7
#| fig-height: 5
#| fig-cap: "Distribution of assymetry and steepness parameters in component GLDs"
#| out-width: "80%"
N <- 400

cfs <- qpd::rMHDR(N, var_ids = 1:2, seed=1)

chi_grd <- cfs[,1]*2-1 #rescale to m1p1
xi_grd <- cfs[,2]

gld_grd <- data.frame(
  chi=chi_grd,
  xi = xi_grd
)
#ggpairs(gld_grd,
#        diag = list(continuous = wrap("barDiag")))
```

```{r}
#| echo: false

# calculate the Q matrix and its derivative
Q <- sapply(seq(nrow(gld_grd)),
  function(i) qpd::qGLDcsw(theta_ecfd_df$p,
                      mu=0, 1,
                      chi=gld_grd$chi[i], xi=gld_grd$xi[i]))
q <- sapply(seq(nrow(gld_grd)),
            function(i) qpd::fGLDcsw(theta_ecfd_df$p,
                    mu=0, 1,
                    chi=gld_grd$chi[i], xi=gld_grd$xi[i]))

# This includes the trick to get possibly non-positive intercept
Qm <- cbind(rep(-1, nrow(Q)), rep(1, nrow(Q)), Q)
colnames(Qm) <- c("negInt", "Int", paste0("V_", seq(ncol(Q))))
qm <- cbind(rep(0, nrow(Q)), rep(0, nrow(Q)), q)
colnames(qm) <- c("negInt", "Int", paste0("V_", seq(ncol(q))))
Y <- theta_ecfd_df$q
stopifnot("NaN in the matrix!"=!any(is.na(Q)))

nn_fit <- nnls(Qm, Y)

component_pars <- tibble(chi=c(NA,NA), xi=c(NA,NA)) |>
  bind_rows(gld_grd) |>
  mutate(coef_x=nn_fit$x,
    par_t=paste0("chi = ", scales::number(chi, 1e-3),
                "; xi = ",scales::number(xi, 1e-3),
           "\n weight = ", scales::number(coef_x, 1e-3))) |>
  slice(nn_fit$passive) |>
  arrange(coef_x) |>
  mutate(par_t=fct_reorder(par_t, -coef_x)) |>
  drop_na()

comp_df <- component_pars |>
  rowid_to_column() |>
  mutate(p=list(theta_ecfd_df$p),
         fitted_Q=map2(chi,xi,
          ~qpd::qGLDcsw(theta_ecfd_df$p, mu=0, sg=1,
                        chi=.x, xi=.y)),
         fitted_q=map2(chi,xi,
          ~qpd::fGLDcsw(theta_ecfd_df$p, mu=0, sg=1,
                                         chi=.x, xi=.y))
  ) |>
  unnest(cols=c(p, fitted_Q, fitted_q)) |>
  mutate(fitted_d =1/fitted_q)

theta_ecfd_df <- theta_ecfd_df |>
  mutate(Q_fitted=nn_fit$fitted,
         q_fitted = qm %*% coef(nn_fit),
         d_fitted = 1/q_fitted)
```

```{r}
#| label: fig-gld-comp
#| out-width: "100%"
#| fig-width: 10
#| fig-height: 8
#| fig-cap: "Density functions and quantile functions of GLD components in approximating quantile mixture"

p1 <- ggplot(comp_df) +
  geom_line(aes(x=fitted_Q, y=fitted_d))+
  facet_wrap(vars(par_t), scales = "free", ncol=2)+
  coord_cartesian(xlim=c(-3,3))+
  labs(y="1/q(u)", x="Q(u)")+
  theme_bw() +
  theme(axis.text.y = element_blank())

p2 <- ggplot(comp_df) +
  geom_line(aes(x=p, y=fitted_Q))+
  facet_wrap(vars(par_t), scales = "free_y", ncol=2)+
  coord_cartesian(ylim=c(-5,5))+
  labs(y="Q(u)", x="u")+
  theme_bw()

p1+p2
```

```{r}
#| label: fig-fitted-qm
#| fig-width: 7
#| fig-height: 5
#| out-width: "80%"
#| fig-cap: "Distribution of posterior samples approximated by the quantile mixture"

ggplot(theta_ecfd_df) +
  geom_histogram(aes(x=q, y=after_stat(density)), bins=40, fill="grey60")+
#  geom_density(aes(x=q, y=after_stat(density)), color="grey20")+
  geom_line(aes(x=Q_fitted, y=d_fitted), color="firebrick", linewidth=1.5)+
  labs(x="theta[5]", y="density")+
  hrbrthemes::theme_ipsum_rc(grid=FALSE)
```

@fig-fitted-qm shows the histogram of 4000 parameter values for `theta[5]` along with the approximation using the quantile mixture with the components shown in @fig-gld-comp. The resulting mixture is a linear combination of GLD quantile functions with a closed form QF and DQF, which makes it possible to reuse this distribution as a quantile-based prior in a Bayesian model [@perepolkin2023TenetsQuantilebasedInference].


## Choosing quantile-parameterized distribution {#sec-compareqf}

A common approach to assess the properties of probability distributions is through central moments, denoted by $\mu_k=\mathbb{E}[(Y-\mu)^k]$, where $\mu$ represents the expected value of $Y$. Karl Pearson introduced a classification system for distributions using moment ratios associated with skewness and kurtosis [@fiori2009KarlPearsonOrigin]:

$$
\beta_1=\frac{\mu_3^2}{\mu_2^3},\quad \beta_2=\frac{\mu_4}{\mu_2^2}
$$

While computing moments using the quantile function is straightforward (the $n$-th raw moment is $\mu_k=\int_0^1Q(u)^kdu$), it may not be possible to calculate higher-order moments for certain distributions.

Alternatively, robust alternatives to moments can be utilized, such as the sample median $\mu_r$, the interquartile range $\sigma_r$, the quartile-based robust coefficient of skewness $s_r$ [@kim2004MoreRobustEstimation], also known as Bowley's skewness [@bowley1920ElementsStatistics] or Galton's skewness [@gilchrist2000StatisticalModellingQuantile], and the octile-based robust coefficient of kurtosis $\kappa_r$, also known as Moors' kurtosis [@moors1988QuantileAlternativeKurtosis].

$$
\begin{aligned}
&\mu_r=Q(1/2)\\
&\sigma_r=Q(3/4)-Q(1/4)\\
&s_r=\frac{Q(3/4)+Q(1/4)-2Q(1/2)}{\sigma_r}\\
&\kappa_r=\frac{Q(7/8)-Q(5/8)+Q(3/8)-Q(1/8)}{\sigma_r}
\end{aligned}
$$

[@kim2004MoreRobustEstimation;@arachchige2022RobustAnalogsCoefficient] have proposed to standardize robust moments to facilitate their comparison with the corresponding robust moments of the standard normal distribution. [@groeneveld1998ClassQuantileMeasures;@jones2011SkewnessInvariantMeasuresKurtosis] have introduced generalizations of robust moments to other quantiles.

Unlike moments, quantiles are always well defined, and since QPDs are parameterized by quantile-probability pairs, quantile-based robust moments can sometimes be directly computed from the parameters. For instance, if the basic quantile function $S(u)$ in $Q(u)=\mu+\sigma S(u)$ is standardized (such that $S(0.5)=0$), where $\mu$ and $\sigma$ are the location and scale parameters of $Q(u)$ respectively, then $\mu_r=\mu$. Moreover, $\sigma_r$ is always independent of location, and $s_r$ and $\kappa_r$ are independent of both location and scale.

@fig-unbounded, @fig-semibounded, and @fig-bounded resemble the Cullen and Frey [@cullen1999ProbabilisticTechniquesExposure] plots (Pearson plots), but instead of using central moments, they employ quartile/octile-based robust metrics of skewness $s_r$ and kurtosis $\kappa_r$ to compare the quantile-parameterized distributions to some of their parametric counterparts. 

In these plots, Metalog3 and Metalog4 refer to 3- and 4-term metalog distributions, respectively, and GLDcsw refers to Chalabi et al [@chalabi2012FlexibleDistributionModeling] parameterization of GLD. As can be seen in @fig-unbounded, all generalizations of Myerson distributions have higher robust kurtosis for the same robust skewness. Additionally, GLD CSW is more flexible than the unbounded 4-term metalog. The *log*-transformed metalog distribution appears to be the best among the semi-bounded distributions (@fig-semibounded). Furthermore, the flexibility of the bounded J-QPD-B is at least as good as that of the Beta and Kumaraswamy distributions (@fig-bounded).  

```{r}
#| label: fig-unbounded
#| fig-cap: "Robust skewness vs robust kurtosis for some unbounded distributions"
#| fig-align: 'center'
#| out-width: "80%"

knitr::include_graphics("img/unbounded_final.png", dpi=200)
```

```{r}
#| label: fig-semibounded
#| fig-cap: "Robust skewness vs robust kurtosis for some left-bounded distributions"
#| fig-align: 'center'
#| out-width: "80%"

knitr::include_graphics("img/semibounded_final.png", dpi=200)
```

```{r}
#| label: fig-bounded
#| fig-cap: "Robust skewness vs robust kurtosis for some bounded distributions"
#| fig-align: 'center'
#| out-width: "80%"

knitr::include_graphics("img/bounded_final.png", dpi=200)
```

# Multivariate quantile-parameterized distributions {#sec-multivariateqpd}

Quantile-parameterized distributions can serve as marginal distributions in multivariate models, where the dependency structure is captured by a standard (parametric) multivariate distribution, a copula, or described by bivariate quantiles. However, the marginal distributions alone are insufficient to determine the corresponding bivariate distribution, resulting in an infinite number of bivariate distributions with the same margins [@gumbel1960BivariateExponentialDistributions; @gumbel1961BivariateLogisticDistributions]. In this section, we describe several methods for extending the distributions parameterized by the quantile-probability pairs to become Multivariate Quantile-Parameterized Distributions (MQPDs).

## MQPDs based on standard multivariate distributions

### Normal distribution

In the simplest case, multivariate Quantile-Parameterized Distributions (MQPDs) can be created by using the multivariate normal distribution, following the approach of [@hoff2007ExtendingRankLikelihood]. The Myerson, J-QPD, and SQN quantile functions are Q-transformations of the probit $Q(z(u)\vert\theta)$, where $z(u)=\Phi^{-1}(u)$ represents the standard normal quantile function. The multivariate versions of these distributions can be viewed as the Q-transformations of the multivariate normal distribution. To extend these QPDs to $J$ dimensions using the multivariate normal distribution, we employ the method outlined in [@drovandi2011LikelihoodfreeBayesianEstimation].

The $i$-th component of a single observation $y_i$ can be described by the quantile function:

$$
y_i=Q(z(u_i)\vert\theta_i), \; \text{for }i=1,\dots,J 
$$

where $\theta_i$ represents the set of parameters for component $i$ (e.g., $\{q_1,q_2,q_3, \alpha\}_i)$ for Myerson or J-QPD distributions). The vector $(z(u_1),\dots,z(u_j))^T\sim N(0,\Sigma)$, where $\Sigma$ denotes the covariance matrix.

For invertible distributions, the inverse quantile function is the cumulative distribution function (CDF) $Q^{-1}(y_i\vert\theta)=F(y_i\vert\theta)$, otherwise, the inverse can be computed numerically as $\widehat{F}(y_i\vert\theta)=\widehat{Q^{-1}}(y_i\vert\theta)$ [@perepolkin2023TenetsQuantilebasedInference].

Drovandi and Pettitt [@drovandi2011LikelihoodfreeBayesianEstimation] show that the joint density of a single (multivariate) observation $(y_i,\dots,y_J)$ can be expressed as:

$$
f(y_1,\dots,y_J\vert\theta)=\varphi(z(Q^{-1}(y_1\vert\theta_1)),\dots,z(Q^{-1}(y_J\vert\theta_J));\Sigma)\prod_{i=1}^{J}\frac{dQ^{-1}(y_i\vert\theta_i)}{dy_i}
$$

where $z(Q^{-1}(y_i\vert\theta_i))=z_i$, $\varphi(z_1,\dots,z_J;\Sigma)$ represents the multivariate normal density with a mean of zero and a covariance matrix of $\Sigma$, and $\frac{dQ^{-1}(y_i)}{dy_i}=f(y_i)$ is the probability density function (PDF) of the QPD (refer to Appendix A).

For distributions without a PDF, the same joint density can be expressed as a joint density quantile function

$$
[q(u_1,\dots,u_j)]^{-1}=\varphi(z(u_1),\dots,z(u_J);\Sigma)\prod_{i=1}^{J}[q(u_i\vert\theta_i)]^{-1}
$$

since $Q^{-1}(y_i\vert\theta_i)=u_i$ and $f(y_i\vert\theta_i)=[q(u_i\vert\theta_i)]^{-1}$ [@gilchrist2000StatisticalModellingQuantile].

It's worth noting that this method of creating multivariate distributions does not require every component to follow the same distributional form. As illustrated earlier, it is entirely possible to combine several different QPDs using the multivariate Gaussian distribution [@drovandi2011LikelihoodfreeBayesianEstimation].

To use the MQPD for the prior, both the density of the multivariate normal and the marginal densities need to be explicitly added to the log-likelihood. This is possible when the marginal QPDs used to define the multivariate prior are invertible, such as Myerson and J-QPD, as both the CDF ($Q^{-1}(y_i\vert\theta_i)$) and PDF ($dQ^{-1}(y_i\vert\theta_i)/dy_i$) are required.

When a quantile-based prior specification is used, only the multivariate normal log-density needs to be added because the Jacobian for the marginal QF transformation is reciprocal to the DQF of the prior [@perepolkin2023TenetsQuantilebasedInference].

### Logistic distribution

The same approach of joining the marginal QPDs can be applied by using the base quantile functions of other distributions. For instance, the Logit-Myerson distribution [@wilson2023ReconciliationExpertPriors] is based on the logistic quantile function. Two Logit-Myerson distributions can be connected using the bivariate logistic distribution. [@gumbel1961BivariateLogisticDistributions] proposed three different formulations for the bivariate logistic distribution. The Type II distribution from the Morgenstern Family [@sajeevkumar2014EstimationParameterMorgenstern; @basikhasteh2021BayesianEstimationMorgenstern] has the following joint distribution and  density functions:

$$
\begin{aligned}
F(y_1,y_2\vert\beta)=&F_1(y_1)F_2(y_2)[1+\beta(1-F_1(y_1))(1-F_2(y_2))]\\
f(y_1,y_2\vert\beta)=&f_1(y_1)f_2(y_2)[1+\beta(1-2F_1(y_1))(1-2F_2(y_2))]
\end{aligned}
$$

where $F_i(y_i)$ and $f_i(y_i)$ for $i\in\{1,2\}$ refer to the univariate logistic distribution and density funcitons, respectively and $-1\leq\beta\leq1$. Since $y_i=Q_i(u_i)$ we can express the bivariate density in the quantile form

$$
\begin{aligned}
f(Q(u_1),Q(u_2)\vert\beta)=&f_1(Q(u_1))f_2(Q(u_2))[1+\beta(1-2F_1(Q_1(u_1)))(1-2F_2(Q_2(u_2)))]\\
\left[q(u_1,u_2\vert\beta)\right]^{-1}=&[q_1(u_1)]^{-1}[q_2(u_2)]^{-1}\left[1+\beta (1-2u_1)(1-2u_2)\right]
\end{aligned}
$$

For logistic distribution $Q(u)=\ln(u)-\ln(1-u)$ and $[q(u)]^{-1}=u(1-u)$. Therefore, the bivariate logistic density quantile function can be expressed as

$$
\left[q_L(u_1,u_2\vert\beta)\right]^{-1}=u_1(1-u_1)u_2(1-u_2)\left[1+\beta (1-2u_1)(1-2u_2)\right]
$$

If we combine the QPD marginals, the result is the joint quantile-based density for the bivariate logistic-based QPD, where the dependency is captured by the bivariate logistic distribution with the coupling parameter $\beta$, and the margins are QPDs. The joint density quantile function is given by:

$$
\begin{aligned}
\left[q_{MQPD}(u_1,u_2\vert\theta_1,\theta_2, \beta)\right]^{-1}=&u_1(1-u_1)u_2(1-u_2)\left[1+\beta (1-2u_1)(1-2u_2)\right]\times\\
&[q_1(u_1\vert\theta_1)]^{-1}[q_2(u_2\vert\theta_2)]^{-1}
\end{aligned}
$$

Here, $[q_i(u_i\vert\theta_i)]^{-1}$, for $i=1,2$, represents the marginal QPD density quantile functions, such as the density quantile function (DQF) of the Logit-Myerson distribution (see Appendix A).

```{r}
#| label: fig-bi-logitmyerson
#| echo: false
#| out-width: "80%"
#| fig-cap: "Density of Generalized Myerson distributions joined by Type II bivariate logistic distribution"
#| fig-align: 'center'
#| warning: false

set.seed(42)
N <- 1e3
# variances on diagonal and covariances off diagonal
bt <- -0.6
myerson_df <- expand.grid(
  u1 = ppoints(N),
  u2 = ppoints(N)) %>% 
  mutate(
  lx1 = qlogis(u1),
  lx2 = qlogis(u2),
  x1 = qlogitMyerson(u1, 3, 7, 10, alpha = 0.1),
  dx1 = dlogitMyerson(u1, 3, 7, 10, alpha = 0.1),
  x2 = qlogitMyerson(u2, -9, -3, 2, alpha = 0.25),
  dx2 = dlogitMyerson(u2, -9, -3, 2, alpha = 0.25), 
  d = dlogis(lx1)*dlogis(lx2)*(1+bt*(1-2*plogis(lx1))*(1-2*plogis(lx2)))*dx1*dx2
 )

ggplot(myerson_df,aes(x1,x2, z=d))+
  geom_contour()+
  geom_vline(xintercept = c(3,7,10), color="grey50", linetype=2)+
  geom_hline(yintercept = c(-9, -3, 2), color="grey50", linetype=2)+
  hrbrthemes::theme_ipsum_rc(grid_col="grey90")+
  labs(x="x ~ Myerson(3,7,10; 0.25)",
       y="y ~ Myerson(-9, -3, 2; 0.10)")

```

@fig-bi-logitmyerson presents the Bivariate Logit-Myerson Distribution, parameterized by $\Theta=\{\theta_1, \theta_2, \rho\}$, where the marginal Myerson distributions are given by $y_{ij}=Q_j(z(u_{ij}),\theta_j)$ for $j=1,2$, with parameter vectors $\theta_1=\{3,7,10;0.25\}$, $\theta_2=\{1,10,20;0.1\}$, and the dependence parameter $\beta=0.6$. 

## Copula-based MQPDs

The approach we have used so far is similar to constructing the joint distribution using the Gaussian copula [@hoff2007ExtendingRankLikelihood]. Copulas provide a general approach to modeling joint distributions, separating the bivariate dependence from the effects of marginal distributions [@kurowicka2006UncertaintyAnalysisHigh]. The literature describes a wide range of copulas [@genest2007EverythingYouAlways; @smith2013BayesianApproachesCopula; @kurowicka2011DependenceModelingVine], and new copulas can be created using generator functions [@durrleman2000SimpleTransformationCopulas]. When a copula is used to connect QPDs, the joint density is calculated as follows:

$$
f_{MQPD}(y_1,y_2\vert \theta_1,\theta_2,\Xi)=c(F(y_1\vert\theta_1),F(y_2\vert\theta_2)\vert\Xi)
f_1\left(y_1\vert\theta_1\right) f_2\left(y_2\vert\theta_2\right)
$$

where $c$ represents the copula density function with parameter $\Xi$, and $F(y_i\vert\theta_i)$ and $f_i(y_i\vert\theta_i)$ are the CDF and PDF of the marginal quantile-parameterized distributions, respectively.

The same density can be expressed in quantile-based form [@perepolkin2023TenetsQuantilebasedInference]:

$$
[q_{MQPD}(u_1,u_2\vert\theta, \Xi)]^{-1}=c\left(u_1,u_2\vert\Xi\right)[q_1(u_1\vert\theta_1)]^{-1}[q_2(u_2\vert\theta)]^{-1}
$$

where $c$ is the copula density function with parameter $\Xi$, and $[q_i(u_i\vert\theta_i)]^{-1}$, for $i=1,2$, are the marginal DQFs of QPDs. @fig-bc-myerson presents 10,000 samples from the bivariate Myerson distribution joined by the Joe copula with $\theta=3$.

Elicitation of multivariate distributions may require a specialized approach [@elfadaly2017ElicitingDirichletGaussian; @wilson2021RecentAdvancesElicitation]. For examples of expert-specified multivariate distributions encoded with copulas, we refer to [@wilson2018SpecificationInformativePrior; @holzhauer2022ElicitingJudgementsDependent; @sharma2018RegularizationVariableSelection; @aas2009PaircopulaConstructionsMultiple]. When fitting copulas to empirical observations, the "blanket" goodness of fit measure [@wang2000ModelSelectionSemiparametric] based on Kendall's transform [@genest2006GoodnessofFitProceduresCopula; @genest2009GoodnessoffitTestsCopulas] can be used.

```{r}
#| label: fig-bc-myerson
#| echo: false
#| out-width: "80%"
#| warning: false
#| error: false
#| message: false
#| fig-cap: "Samples from the bivariate Myerson distribution joined by the Joe copula ($\\theta=3$)"
#| fig-align: 'center'


library(rvinecopulib)
set.seed(42)
N <- 1e4
smpls_u <- rvinecopulib::rbicop(N, family="joe", parameters=c(3))

myerson_df <- tibble(
x1 = qMyerson(smpls_u[,1], 3, 7, 10, alpha = 0.25),
x2 = qMyerson(smpls_u[,2], -9, -3, 2, alpha=0.1)
)

myerson_kde <- MASS::kde2d(myerson_df$x1, myerson_df$x2, n = 100) 

p1 <- ggplot(myerson_df,aes(x1,x2))+
  geom_point(alpha=0.1)+
  geom_density2d()+
  geom_vline(xintercept = c(3,7,10), color="grey30", linetype=2)+
  geom_hline(yintercept = c(-9, -3, 2), color="grey30", linetype=2)+
  hrbrthemes::theme_ipsum_rc(grid_col="grey90")+
  labs(x="x ~ Myerson(3,7,10; 0.25)",
       y="y ~ Myerson(-9, -3, 2; 0.10)")

ggMarginal(p1, type="densigram", color="royalblue", fill="grey30")

```

## Bivariate quantiles

The formal definition of bivariate quantile functions and the method for constructing bivariate quantile distributions using marginal and conditional quantile functions are provided by [@nair2023PropertiesBivariateDistributions;@vineshkumar2019BivariateQuantileFunctions]. They define the bivarate quantile function (bQF) of $(X_1, X_2)$ as the pair 
$Q(u_1, u_2)=(Q_1(u_1), Q_{21}(u_2\vert u_1))$, where $Q_1(u_1)=\inf \{x_1: F_1(x_1)\geq u_1\}$,  $u_1\in[0,1]$ and $Q_{21}(u_2\vert u_1)=\inf\{x_2: F_{21}(Q_1, x_2)\geq u_2\}$.

The conditional quantile function $Q_{21}(u_2\vert u_1)$ can be obtained by inverting the conditional distribution function $F_{21}(x_1, x_2)$, which is computed from the factorization of the joint survival function. The joint survival function is defined as $\bar{F}(x_1, x_2)=P(X_1> x_1)P(X_2> x_2 \vert X_1 > x_1)= \bar{F}(x_1)\bar{F}_{21}(x_1,x_2)$. Note that the joint survival function $\bar{F}(x_1,x_2)=1-F_1(x_1)-F_2(x_2)+F(x_1,x_2)$, and the conditional survival function $\bar{F}_{21}(x_1,x_2)=1-F_{21}(x_1,x_2)$.

Another approach for creating bivariate quantile functions is through Gilchrist's QF transformation rules [@gilchrist2000StatisticalModellingQuantile], which can be generalized to bivariate quantile functions. According to [@nair2023PropertiesBivariateDistributions] (Property 6), the conditional QF can be constructed as a sum of two univariate QFs: $Q_{21}(u_2\vert u_1) = Q_1(u_1) + Q_2(u_2)$. This means that the pair $(Q_1(u_1), ; Q_1(u_1) + Q_2(u_2))$ is a valid bivariate quantile function, which generalizes Gilchrist's *addition rule* (@tbl-qf-trans). The addition rule also works for quantile density functions (Property 7). If $Q_1$ is left-bounded at zero, i.e., $Q_1(0) = 0$, then the margins of such a bQF are $X_1 = Q_1(u_1)$ and $X_2 = Q_2(u_2)$. Otherwise, the marginal distribution of $X_2$ will be $\lim_{u_1 \rightarrow 0}Q_{21}(u_2\vert u_1)$, which in many cases is not tractable.

If $Q_1(u_1)$ and $Q_2(u_2)$ are positive on $u_i \in [0,1]$, then their product is also a valid conditional QF (Property 8), generalizing Gilchrist's "product rule". Finally, Property 9 generalizes the "Q-transformation rule," stating that for every increasing transformation functions $T_1$ and $T_2$, $\left(T_1(Q_1(u_1)), T_1(Q_1(u_1)) + T_2(Q_2(u_2))\right)$ is also a valid bQF.

Therefore, valid bivariate quantile-parameterized QFs can be created by constructing the conditional quantile functions as Gilchrist combinations of univariate quantile-parameterized QFs. @fig-bq-myerson shows 1000 samples from the bivariate distribution created by adding together two Myerson distributions. Note that in this case, only the marginal distribution of $x_1 = Q_1(u_1)$ is available in closed form.

$$
\begin{aligned}
(u_1, u_2) &\overset{X_1, X_2}{\backsim} (Q_1(u_1), Q_1(u_1)+Q_2(u_2))\\
Q_1(u_1) &\sim\text{Myerson}(3,7,10; 0.1)\\
Q_2(u_2) &\sim \text{Myerson}(-9, -3, 2; 0.25)\\
\end{aligned}
$$

This bQF is easy to elicit and interpret, since $Q_2(u_2)$ can be thought of as a random adjustment to the value of $Q_1(u_1)$. In fact, the conditional quantile function $Q_{21}(u_2\vert u_1)$ can be thought of as having the classical form $Q_{21}(u_2\vert u_1) = \mu(u_1) + \sigma Q_2(u_2)$ [@gilchrist2000StatisticalModellingQuantile], where the location is randomly varying with $\mu(u_1) = Q_1(u_1)$ and the scale parameter $\sigma = 1$. First, the marginal distribution $Q_1(u_1)$ is elicited, and then the difference between the values $x_1$ and $x_2$ can be elicited as a QPT and encoded as $Q_2(u_2)$.

```{r}
#| label: fig-bq-myerson
#| echo: false
#| out-width: "80%"
#| fig-cap: "Samples from the Bivariate Myerson quantile function"
#| fig-align: 'center'
#| warning: false

n <- 1e4
set.seed(42)
bq_df <- tibble::tibble(
  u1=runif(n),
  u2=runif(n),
  x1=qMyerson(u1, 3,7,10, 0.25),
  x2=x1+qMyerson(u2, -9, -3, 2, 0.1)
)
p1 <- ggplot(bq_df, aes(x1, x2))+
    geom_point(alpha=0.1)+
    geom_density2d()+
    geom_vline(xintercept = c(3,7,10), color="grey30", linetype=2)+
    hrbrthemes::theme_ipsum_rc(grid_col="grey90")+
    labs(x="x1 ~ Myerson(3,7,10; 0.25)")

ggMarginal(p1, type="densigram", color="royalblue",
           fill="grey30")
```

# Discussion

Quantile-based distributions have garnered significant attention in the research community. Several distributions, such as the Generalized Lambda Distribution (GLD)  [@freimer1988StudyGeneralizedTukey; @ramberg1974ApproximateMethodGenerating], the g-and-k distribution [@haynes1997RobustnessRankingSelection; @haynes2005BayesianEstimationGandk; @jacob2017LikelihoodCalculationGandk;@prangle2017GkPackageGandk], the g-and-h distribution [@field2006MultivariateGandhDistribution; @macgillivray1992ShapePropertiesGandh; @rayner2002NumericalMaximumLikelihood], and the Wakeby distribution [@jeong-soo2005WakebyDistributionMaximum; @rahman2015ApplicabilityWakebyDistribution; @tarsitano2005FittingWakebyModel], have been extensively studied and documented in the literature. These distributions are defined by non-invertible quantile functions [@perepolkin2023TenetsQuantilebasedInference]. However, the research on quantile-parameterized distributions remains relatively unexplored. These distributions offer interpretable parameters that are defined on the same scale as the quantities of interest, simplifying the elicitation process for experts. Many popular elicitation protocols for both predictive and parametric elicitation rely on the assessment of quantile-probability pairs (QPPs). Instead of fitting a parametric distribution to the elicited QPPs [@best2020PriorElicitation; @ohagan2019ExpertKnowledgeElicitation], assessors could directly use the elicited QPPs as inputs into one of the QPD quantile functions, which can be easily employed in both quantile-parameterized and parametric models.

Provided that the expert and the elicitor agree on the scientific model to be used for representing the expert's understanding of the world [@burgman2021ElicitingModelStructures], several types of inputs may be required to inform the model. Among those are the expert's judgement about the model *parameters* [@mikkola2021PriorKnowledgeElicitation; @ohagan2019ExpertKnowledgeElicitation] and their *predictions* of the next observation [@akbarov2009ProbabilityElicitationPredictive; @kadane1998ExperiencesElicitation; @winkler1980PriorInformationPredictive]. Both parametric and predictive judgments should be captured together with corresponding uncertainties to reflect the expert's state of knowledge. Quantile-parameterized distributions offer distinct advantages as high-fidelity priors that precisely capture expert assessments. These distributions are particularly beneficial for domain experts who may not be well-versed in statistics, as they provide high flexibility while retaining parameter interpretability. As a result, QPDs can faithfully represent an expert's beliefs without compromising convenience or precision. 

Different quantile-parameterized distributions fitted to the same set of quantile-probability pairs may exhibit slight variations in shape. However, given the diverse range of QPDs proposed in the literature a knowledgeable assessor should be able to select an appropriate distribution and validate the choice with the expert, taking into account the thickness of the distribution tails.

Most QPDs we reviewed are parameterized by a symmetric percentile triplet (SPT). These distributions rely on the symmetric property of underlying *kernel* distributions and can be generalized by swapping the distribution with another one that exhibits different tail shapes. Hadlock and Bickel [@hadlock2019GeneralizedJohnsonQuantileParameterized] utilized this method to generalize Johnson Quantile Parameterized distributions (J-QPDs). We show that the variants of Myerson distribution appearing in the literature [@myerson2005ProbabilityModelsEconomic; @wilson2023ReconciliationExpertPriors] represent similar generalization. This principle can be extended to include other kernels which result in varying thickness of the tails.

### Quantile function perspective {.unnumbered}

The distributions discussed in this paper are defined using the quantile function and, therefore, they can be considered *quantile-based* quantile-parameterized distributions. Myerson, J-QPD, and several other quantile-parameterized distributions reparameterize conventional distributions, utilizing Gilchrist's Quantile Function (QF) transformations [@gilchrist2000StatisticalModellingQuantile]. 

Perepolkin et al. [@perepolkin2023TenetsQuantilebasedInference] demonstrated that the distributions defined by the quantile function can be used both as prior and as likelihood in Bayesian models. Priors defined by the quantile function eliminate the need to compute prior density. The quantile function acts as a non-linear transformation of a uniform degenerate random variate with the resulting Jacobian adjustment reciprocal to the density quantile function. Therefore, both the Jacobian and the density quantile function are omitted from the Bayesian updating equation  [@perepolkin2023TenetsQuantilebasedInference]. When using quantile-based QPDs as likelihood, special care needs to be taken with regards to the suitable prior for the QPP parameters. [@perepolkin2021HybridElicitationIndirect] used the Dirichet-based prior for the metalog likelihood model and descibed the *hybrid* elicitation process for encoding the expert judgments into the two-dimensional prior distribution implied by the model.

### Feasibility of parameters {.unnumbered}

Not all QPDs are equally reliable in approximating the underlying distributions. Violating the QF transformation rules imposes additional constraints on the feasibility of parameters, as certain combinations of parameters may result in locally decreasing quantile functions [@keelin2016MetalogDistributions; @hadlock2017QuantileparameterizedMethodsQuantifying]. We discussed this limitation in relation to SQN and metalog distributions, but the same challenges affect other distributions with QF violating Gilchrist QF transformation rules. In this regard, the quantile-parameterized model, which relies on Gilchrist combination of basic quantile functions, proposed by [@peng2023MixtureQuantilesEstimated], represents a highly promising advancement. Weighted constrained optimization algorithm ensuring that the quantile mixture weights remain non-negative opens new possibilities for other QPDs using monotonic transformations of quantile functions. The estimator proposed by [@peng2023MixtureQuantilesEstimated] is asymptotically a q-Wasserstein distance, which has also been used for parameter estimation in Approximate Bayesian Computation [@bernton2019ParameterEstimationWasserstein]. 

The feasibility conditions for the Generalized Lambda Distribution (GLD) have been a focal point of numerous research endeavors in the past [@dean2013ImprovedEstimationRegression; @fournier2007EstimatingParametersGeneralized; @karian2019FittingStatisticalDistributions; @king2007FittingGeneralizedLambda; @tarsitano2005EstimationGeneralizedLambda, etc]. Various reparameterizations have been explored to enhance parameter identifiability [@ramberg1974ApproximateMethodGenerating]. Recently, [@chalabi2012FlexibleDistributionModeling] proposed a novel asymmetry-skewness reparameterization for the previously popular Freimer, Kollia, Mudholkar & Lin version of GLD (FKML GLD) [@freimer1988StudyGeneralizedTukey], wherein two of the four parameters are mapped to robust quantile-based moments, namely the median and Interquartile Range (IQR). This reduction in the number of parameters required for data fitting simplifies the previously computationally intensive fitting algorithms. As demonstrated in the plot of robust moments (@fig-unbounded) GLD remains one of the most flexible unbounded distributions, capable of accommodating a wide range of shapes. [@dedduwakumara2021EfficientEstimatorParameters] described a two-step method for fitting FKML GLD using the probability density quantile function [@staudte2017ShapesThingsCome]. However, when applying their method to fitting the CSW GLD, the second step becomes unnecessary as the location and scale can be directly mapped to the empirical first and second robust moments. 

CSW GLD represents a prime example of clever reparameterization aiming at alleviating the deficiencies of QF construction through setting consistent parameter boundaries and defining fall-back cases for an impossible combination of parameters. This degree of reparameterization is difficult for QPDs because the objective is to retain the mapping of parameters to the valid set of quantile-probability pairs. Therefore, for improperly constructed QPDs the feasibility conditions will have to be expressed as ratios of quantiles. 

### Multivariate extensions {.unnumbered}

Quantile-parameterized distributions can be readily extended to the multivariate setting by leveraging traditional multivariate distributions. The combination of quantile-based marginal distributions joined by the multivariate normal has been previously discussed in the literature  [@drovandi2011LikelihoodfreeBayesianEstimation; @hoff2007ExtendingRankLikelihood]. Building on this approach, we proposed the use of Gumbel's bivariate logistic distribution [@gumbel1961BivariateLogisticDistributions] to combine quantile-parameterized Logit-Myerson distributions [@wilson2023ReconciliationExpertPriors].

Copulas offer a natural extension of univariate QPDs into the multivariate domain. Bivariate copulas can be assembled into more complex structures using vine copulas [@czado2019AnalyzingDependentData; @kurowicka2011DependenceModelingVine; @wilson2018SpecificationInformativePrior]. Flexible QPDs serve as a viable alternative to empirical copulas, where the margins are represented by kernel density estimation (KDE) or other non-parametric approaches. Poorly fitted marginal distributions mean *less-than-ideal* starting point for copula modeling, because of deviations from uniformality of the copula margins.

Quantile-parameterized distributions defined by the quantile function are particularly well-suited for constructing new distributions using bivariate quantiles [@nair2023PropertiesBivariateDistributions; @vineshkumar2019BivariateQuantileFunctions]. The ability to construct a conditional quantile function as a Gilchrist combination of univariate quantile functions offers a convenient and interpretable approach to defining bivariate distributions, especially when the univariate quantile functions are parameterized by quantiles. These distributions are easy to sample from and construct. However, fitting these distributions to data or posterior samples can be challenging. As shown by [@castillo1997FittingContinuousBivariate] the fitting process requires all marginal and conditional quantile functions to be available in closed form, which is often unattainable.

### Further research {.unnumbered}

There appears to be a limited availability of unbounded quantile-parameterized distributions in the current literature. Among the distributions we examined, only the metalog distribution and quantile mixtures can extend across the entire real line. The G-QPD system provides clear distributional bounds explicitly defined by the expert during elicitation. In contrast, the (Generalized) Myerson distribution system relies on implicit bounds that need to be communicated to the expert. Most of the distributions we reviewed are characterized by a symmetrical percentile triplet (SPT), as they rely on the symmetrical property of their kernels. However, there may be situations where an arbitrary (non-symmetrical) quantile parameterization could prove valuable [as shown by @perepolkin2021HybridElicitationIndirect]. The development of flexible quantile-parameterized distributions defined by an arbitrary set of quantile-probability pairs using quantile mixtures [@peng2023MixtureQuantilesEstimated] can enhance versatility of QPDs and facilitate their broader adoption.

In conclusion, quantile-parameterized distributions offer a valuable framework for capturing expert assessments and incorporating them into statistical models. They provide high flexibility and parameter interpretability, making them particularly beneficial for domain experts. The diverse range of quantile-parameterized distributions explored in the literature allows for customized modeling approaches that align with the expert's beliefs and uncertainties. By embracing these innovative distributions, researchers and practitioners can enhance the accuracy and reliability of their statistical models while leveraging expert knowledge effectively.

# Miscellaneous{-}

## Acknowledgments{-}

The authors have no conflict of interest to declare. U Sahlin was funded by the Crafoord Foundation (ref 20200626). We thank the editorial team and reviewers for their constructive feedback which helped us improve this manuscript.

## ORCID{-}
Dmytro Perepolkin https://orcid.org/0000-0001-8558-6183   
Erik Lindström https://orcid.org/0000-0002-6468-2624  
Ullrika Sahlin http://orcid.org/0000-0002-2932-6253  

# Appendix A. Distribution functions {.appendix .unnumbered}

## Myerson Distribution {.unnumbered}

The derivative of the quantile function with respect to the depth $u$ is the Quantile Density Function, which for Myerson distribution has the following form

$$
q(u\vert q_1,q_2,q_3,\alpha)=\begin{cases}
\rho\frac{\beta^\kappa\ln(\beta)}{(\beta-1)}\frac{q_{norm}(u)}{\Phi^{-1}(1-\alpha)}, \quad &\beta \neq 1\\
\rho\frac{q_{norm}(u)}{\Phi^{-1}(1-\alpha)}, \quad &\beta = 1
\end{cases}
$$

where $q_{norm}=\frac{d\Phi^{-1}(u)}{du}$ is the quantile density function for the standard normal distribution.

The Myerson distribution is invertible. The distribution function of random variable $X$ has the form

$$
\begin{aligned}\;
\psi&=\Phi^{-1}(1-\alpha)\left(\frac{\ln\left(1+\frac{(x-q_2)(\beta-1)}{\rho}\right)}{\ln(\beta)}\right)&\\
F(x\vert q_1, q_2, q_3, \alpha)&=\begin{cases}
\Phi(\psi), \quad &\beta\neq 1\\
F_{normal}(x\vert q_2,\rho/\Phi^{-1}(1-\alpha) ), \quad &\beta=1
\end{cases}
\end{aligned}
$$

where $\Phi()$ is the CDF of the standard normal distribution and $\Phi^{-1}()$ is its inverse. $F_{normal}(x\vert q_2,\rho/\Phi^{-1}(1-\alpha))$ is the CDF of the normal distribution with mean $\mu=q_2$ and standard deviation $\sigma=\rho/\Phi^{-1}(1-\alpha)$.

The derivative of the distribution function with respect to the random variable $X$ is the probability density function, which for the Myerson distribution takes the following form

$$
f(x\vert q_1, q_2, q_3, \alpha)=\begin{cases}
\frac{\Phi^{-1}(1-\alpha)(\beta-1)}{(\rho+(x-q_2)(\beta-1))\ln(\beta)}\varphi(\psi), \quad &\beta\neq1\\
f_{normal}(x\vert q_2,\rho/\Phi^{-1}(1-\alpha)),\quad &\beta=1
\end{cases}
$$

where $\varphi()$ is the probability density function of the standard normal distribution, $f_{normal}(x\vert q_2,\frac{\rho}{\Phi^{-1}(1-\alpha)})$ is the PDF of the normal distribution with the mean $\mu=q_2$ and standard deviation $\sigma=\rho/\Phi^{-1}(1-\alpha))$.


## Generalized Myerson Distributions {.unnumbered}

The Quantile Density Function of Generalized Myerson Distribution for $u\neq0, u\neq1$ is 

$$
q_M(u\vert q_1,q_2,q_3,\alpha)=\begin{cases}
\rho\frac{\beta^\kappa\ln(\beta)}{(\beta-1)}\frac{s(u)}{S(1-\alpha)}, \quad &\beta \neq 1\\
\rho\frac{s(u)}{S(1-\alpha)}, \quad &\beta = 1
\end{cases}
$$

where $S(u)$ is the quantile function and $s(u)=\frac{dS(u)}{du}$ is the quantile density function for the kernel distribution. When $u=0$ or $u=1$ the $q_M(u)=\infty$.

The Generalized Myerson distribution is invertible. The distribution function of random variable $X$ has the form

$$
\begin{aligned}\;
\psi &=S(1-\alpha)\left(\frac{\ln\left(1+\frac{(x-q_2)(\beta-1)}{\rho}\right)}{\ln(\beta)}\right)\\
F_M(x\vert q_1, q_2, q_3, \alpha)&=\begin{cases}
F(\psi), \quad &\beta\neq 1\\
q_2+ \frac{\rho}{S(1-\alpha)}F(x), \quad &\beta=1
\end{cases}
\end{aligned}
$$

where $F()$ is the standard CDF of the kenel distribution and $S()$ is its inverse.

The derivative of the distribution function with respect to the random variable $X$ is the probability density function, which for the Myerson distribution takes the following form

$$
f_M(x\vert q_1, q_2, q_3, \alpha)=\begin{cases}
\frac{S(1-\alpha)(\beta-1)}{(\rho+(x-q_2)(\beta-1))\ln(\beta)}f(\psi), \quad &\beta\neq1\\
f\left(\frac{x-q_2}{\rho/S(1-\alpha)}\right),\quad &\beta=1
\end{cases}
$$

where $f()$ is the probability density function of the standard kernel distribution. Compare it to the simplicity of the Quantile Density Function above. 

## Johnson Quantile-Parameterized  Distribution {.unnumbered}

The JQPD-B quantile density function can be computed as

$$
q_B(p)=\begin{cases}
(u_b-l_b)\varphi(\xi+\lambda\sinh(\delta(z(p)+nc))) \times\\
\quad \lambda\cosh(\sigma(z(p)+nc)) \sigma q_{norm}(p), \quad &n\neq 0\\
(u_b-l_b)\varphi\left(B+\left(\frac{H-L}{2c}\right)z(p)\right)
\times \left(\frac{H-L}{2c}\right)q_{norm}(p), \quad &n=0
\end{cases}
$$

The JQPD-B distribution function

$$
F_B(x)=\begin{cases}
\Phi\left((2c/(H-L))(-B+z\left(\frac{x-l}{u-l}\right))\right), \quad &n=0 \\
\Phi\left(\frac{1}{\delta}\sinh^{-1}\left(\frac{1}{\lambda}\left(z\left(\frac{x-l}{u-l}\right)-\xi\right)\right)-nc\right), \quad &n\neq0
\end{cases}
$$

The JQPD-B probability density function (PDF) is   

$$
\begin{gathered}
f(x)=\begin{cases}
\frac{2c}{(H-L)(u_b-l_b)}\frac{1}{\varphi\left(z\left(\frac{x-l_b}{u_b-l_b}\right)\right)}\varphi\left(\frac{2c}{H-L}\left(-B+z\left(\frac{x-l_b}{u-l_b}\right)\right)\right), \quad &n=0\\
\frac{1}{\delta}\frac{1}{u_b-l_b}\varphi\left(-nc+\frac{1}{\delta}\sinh^{-1}\left(\frac{1}{\lambda}\left(-\xi+z\left(\frac{x-l_b}{u_b-l_b}\right)\right)\right)\right) \times \\ \indent
\frac{1}{\varphi\left(z\left(\frac{x-l_b}{u_b-l_b}\right)\right)}\frac{1}{\sqrt{\lambda^2+\left(-\xi+z\left(\frac{x-l_b}{u_b-l_b}\right)\right)^2}}, \quad &n\neq 0\\
\end{cases}
\end{gathered}
$$

J-QPD-S quantile density function

$$
q_S(p)=\begin{cases}
\theta\exp\left(\lambda\delta z(p)\right)\lambda\delta q_{norm}(p), \quad &n=0\\
\theta\exp\left(\lambda\sinh^{-1}(\delta z(p))+\sinh^{-1}(nc\delta)\right)\lambda\frac{1}{\sqrt{1+(\delta z(p))^2}}\delta q_{norm}(p), \quad &n\neq0\\
\end{cases}
$$

J-QPD-S distribution function

$$
F_S(x)=\begin{cases}
F_{lnorm}(x-l_b\vert \ln(\theta), \frac{H-B}{c}), \quad &n=0\\
\Phi\left(\frac{1}{\delta}\sinh\left(\sinh^{-1}\left(\frac{1}{\lambda}\ln\frac{x-l_b}{\theta}\right)-\sinh^{-1}(nc\delta)\right)\right), \quad &n\neq0\\
\end{cases}
$$

J-QPD-S probability density function (PDF)

$$
f_S(x)=\begin{cases}
\frac{1}{x\sigma\sqrt{2\pi}}\exp\left(-\frac{(\ln x-ln\xi)^2}{2\frac{(H-B)^2}{c^2}}\right), \quad &n=0\\
\varphi\left(\frac{\sinh(\sinh^{-1}(cn\sigma)-\sinh^{-1}(\frac{1}{\lambda}\ln\frac{x-l_b}{\theta}))}{\delta}\right)\frac{\cosh(\sinh^{-1}(cn\delta)-\sinh^{-1}(\frac{1}{\lambda}\ln\frac{x-l_b}{\theta}))}{(x-l_b)\delta\lambda\sqrt{1+\left(\frac{\ln\frac{x-l_b}{\theta}}{\lambda}\right)^2}}, \quad &n \neq 0
\end{cases}
$$

where $\mu=\ln\xi$ and $\sigma=\frac{H-B}{c}$.

## Metalog distribution {.unnumbered}

This section recapitulates ideas and formulas provided in [@keelin2016MetalogDistributions] with our own notation and minor reinterpretations.

Metalog distribution is created from the logistic quantile function $Q(p)=\mu+s\text{logit}(p)$, where $\mu$ is the mean, $s$ is proportional to the standard deviation such that $\sigma=s\pi/\sqrt3$, $p$ is the probability $p \in [0,1]$. The metalog quantile function is built by substitution and series expansion of its parameters $\mu$ and $s$ with the polynomial of the form:

$$
\begin{aligned}\;
&\mu=a_1+a_4(p-0.5)+a_5(p-0.5)^2+a_7(p-0.5)^3+a_9(p-0.5)^4+\dots, \\
& s=a_2+a_3(p-0.5)+a_6(p-0.5)^2+a_8(p-0.5)^3+a_{10}(p-0.5)^4+\dots,
\end{aligned}
$$

where $a_i, \; i \in (1\dots n)$ are real constants. Given a size-$m$ QPT $\{p, q\}_m$, where $p=\{p_1\dots p_m\}$ and $q=\{q_1\dots q_m\}$ the vector of coefficients $a=\{a_1\dots a_m\}$ can be determined through the set of linear equations.

$$
\begin{aligned}\;
&q_1=a_1+a_2\text{logit}(p_1)+a_3(p_1-0.5)\text{logit}(p_1)+a_4(p_1-0.5)+\cdots,\\
&q_2=a_1+a_2\text{logit}(p_2)+a_3(p_2-0.5)\text{logit}(p_2)+a_4(p_2-0.5)+\cdots,\\
&\vdots\\
&q_m=a_1+a_2\text{logit}(p_m)+a_3(p_m-0.5)\text{logit}(p_m)+a_4(p_m-0.5)+\cdots.\\
\end{aligned}
$$

In the matrix form, this system of equations is equivalent to $q=\mathbb{P}a$, where $q$ and $a$ are column vectors and $\mathbb{P}$ is a $m \times n$ matrix:

$$
\mathbb{P} = \left[\begin{array}{lllll}
1  &\text{logit}(p_1) &(p_1-0.5)\text{logit}(p_1) &(p_1-0.5) &\cdots\\
1  &\text{logit}(p_2) &(p_2-0.5)\text{logit}(p_2) &(p_2-0.5) &\cdots\\
   &                  &\vdots\\
1  &\text{logit}(p_m) &(p_m-0.5)\text{logit}(p_m) &(p_m-0.5) &\cdots
\end{array}\right]
$$

If $m=n$ and $\mathbb{P}$ is invertible, then the vector of coefficients $a$ of this *properly parameterized* metalog QPD can be uniquely determined by 

$$
a=\mathbb{P}^{-1}q
$$ {#eq-nmetalogAsMatrixeq} 

If $m > n$ and $\mathbb{P}$ has a rank of at least $n$, then the vector of coefficients $a$ of the *approximated* metalog QPD, can be estimated using 

$$
a=[\mathbb{P}^T\mathbb{P}]^{-1}\mathbb{P}^Tq
$$

The matrix to be inverted is always $n \times n$ regardless of the size $m$ of QPT used. 

Metalog *quantile function* (QF) with $n$ terms $Q_{M_n}(u\vert a)$ can be expressed as

$$
Q_{M_n}(u\vert a)=\begin{cases}
a_1+a_2\text{logit}(u), \text{ for } n=2, \\
a_1+a_2\text{logit}(u)+a_3(u-0.5)\text{logit}(u), \text{ for } n=3, \\
a_1+a_2\text{logit}(u)+a_3(u-0.5)\text{logit}(u)+a_4(u-0.5), \text{ for } n=4, \\
Q_{M_{n-1}} + a_n(u-0.5)^{(n-1)/2}, \text{ for odd } n \geq 5, \\
Q_{M_{n-1}} + a_n(u-0.5)^{n/2-1}\text{logit}(u), \text{ for even } n \geq 6, \\
\end{cases}
$$ {#eq-metalogQFeq}

where $u \in [0,1]$ is the cumulative probability and $a$ is the size-$n$ parameter vector of real constants $a=\{a_1\dots a_n\}$.

The metalog *quantile density function* (QDF) can be found by differentiating the @eq-metalogQFeq with respect to $u$:

$$
\begin{gathered}
q_{M_n}(u\vert a)=\begin{cases}
a_2\mathcal I(u), \text{ for } n=2, \\
a_2\mathcal I(u)+a_3\left((u-0.5)\mathcal I(u)+\text{logit}(u) \right), \text{ for } n=3, \\
a_2\mathcal I(u) + a_3\left((u-0.5)\mathcal I(u)+\text{logit}(u) \right)+ a_4,  \text{ for } n=4, \\
q_{M_{n-1}} + 0.5a_n(n-1)(u-0.5)^{(n-3)/2}, \text{ for odd } n \geq 5, \\
q_{M_{n-1}} + a_n((u-0.5)^{n/2-1}\mathcal I(u)+\\ \indent (0.5n-1)(u-0.5)^{n/2-2}\text{logit}(u)), \text{ for even } n \geq 6, \\
\end{cases}
\end{gathered}
$$ {#eq-metalogQDFeq}

where $\mathcal I(u)=[u(1-u)]^{-1}$. The constants $a$ are feasible iif $q_{M_{n}}(u\vert a)>0, \;\forall u \in [0,1]$.

Metalog *density quantile function* (DQF), referred to as the "metalog pdf" in [@keelin2016MetalogDistributions] can be obtained by $f(Q_{M_n}(u\vert a))=[q_{M_n}(u\vert a)]^{-1}$.

Metalog *cumulative distribution function* (CDF) $F_{M_n}(x\vert a)$ does not have an explicit form because $Q_{M_n}(u\vert a)$ is not invertible [@keelin2016MetalogDistributions]. It is, however, possible to approximate $\widehat Q^{-1}_{M_n}(x\vert a)$ using approximation.

Metalog distribution is defined for all $x \in \mathbb R$ on the real line. [@keelin2016MetalogDistributions] provides semi-bounded *log-metalog*, and the bounded *logit-metalog* variations of the metalog distribution. As the names suggest, this is achieved through the variable substitution with $z=\ln(x-b_l)$ or $z=-\ln(b_u-x)$ for the semi-bounded case, and $z=\ln((x-b_l)/(b_u-x))$ for the bounded case, where $z$ is metalog-distributed and $b_l, b_u$ are the lower and upper limits, respectively. Substituting one of the transformations into the QF and QDF functions above, yields semi-bounded or bounded metalog distribution. For the exact formulae of the log-metalog and logit-metalog refer to [@keelin2016MetalogDistributions].


## CSW GLD {.unnumbered}

Quantile density function for the CSW GLD is provided in [@chalabi2012FlexibleDistributionModeling]

$$
\begin{gathered}
q(u\vert\tilde\sigma,\chi,\xi)= \frac{\tilde\sigma}{S(0.75\vert\chi,\xi)-S(0.25\vert\chi,\xi)}
s(u\vert\chi,\xi) \\
s(u\vert\chi,\xi)=\frac{d}{du}S(u\vert\chi,\xi)=u^{\alpha+\beta-1}+(1-u)^{\alpha-\beta-1}
\end{gathered}
$$


{{< pagebreak >}}

# References {.unnumbered}

::: {#refs}
:::
